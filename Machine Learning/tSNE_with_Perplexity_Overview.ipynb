{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46504c2c",
   "metadata": {},
   "source": [
    "\n",
    "# t-Distributed Stochastic Neighbor Embedding (t-SNE) with Perplexity Overview\n",
    "\n",
    "This notebook provides an overview of t-Distributed Stochastic Neighbor Embedding (t-SNE), focusing on the role of perplexity, its mathematical foundation, and a basic implementation using a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4abd39",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE is a nonlinear dimensionality reduction technique primarily used for the visualization of high-dimensional datasets. It converts similarities between data points into joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "### Perplexity in t-SNE\n",
    "\n",
    "Perplexity is a crucial hyperparameter in t-SNE that can be thought of as a measure of the effective number of neighbors. It balances attention between local and global aspects of the data. A low perplexity focuses more on local structure, while a high perplexity captures more of the global structure.\n",
    "\n",
    "### Applications of t-SNE\n",
    "\n",
    "t-SNE is widely used for visualizing high-dimensional data in fields like bioinformatics, speech processing, and natural language processing. It is particularly effective for visualizing clusters and understanding the structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcf3f2",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### t-SNE Algorithm\n",
    "\n",
    "t-SNE involves the following steps:\n",
    "\n",
    "1. **Compute Pairwise Similarities**:\n",
    "   - In the high-dimensional space, the similarity between two points \\( x_i \\) and \\( x_j \\) is computed using a Gaussian distribution:\n",
    "\n",
    "\\[\n",
    "P_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
    "\\]\n",
    "\n",
    "   - The perplexity \\( \\text{Perp}(P_i) \\) is related to the variance \\( \\sigma_i \\) and is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{Perp}(P_i) = 2^{-\\sum_j P_{j|i} \\log_2 P_{j|i}}\n",
    "\\]\n",
    "\n",
    "2. **Define Joint Probabilities**:\n",
    "   - The joint probability distribution over pairs is symmetric:\n",
    "\n",
    "\\[\n",
    "P_{ij} = \\frac{P_{j|i} + P_{i|j}}{2n}\n",
    "\\]\n",
    "\n",
    "3. **Low-Dimensional Mapping**:\n",
    "   - In the low-dimensional space, the similarity between two points \\( y_i \\) and \\( y_j \\) is computed using a Student-t distribution:\n",
    "\n",
    "\\[\n",
    "Q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
    "\\]\n",
    "\n",
    "4. **Minimize Kullback-Leibler Divergence**:\n",
    "   - The goal is to minimize the divergence between the joint probabilities \\( P_{ij} \\) and \\( Q_{ij} \\):\n",
    "\n",
    "\\[\n",
    "\\text{KL}(P \\| Q) = \\sum_{i \\neq j} P_{ij} \\log \\frac{P_{ij}}{Q_{ij}}\n",
    "\\]\n",
    "\n",
    "### Effect of Perplexity\n",
    "\n",
    "Perplexity influences the bandwidth of the Gaussian kernel used to compute pairwise similarities. A small perplexity emphasizes local relationships, leading to more detailed clusters, while a large perplexity captures broader, more global structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69716303",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement t-SNE using Scikit-Learn on the Iris dataset and explore the effects of different perplexity values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Apply t-SNE with different perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "    plt.title(f\"t-SNE with Perplexity={perplexity}\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ef40b",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an overview of t-Distributed Stochastic Neighbor Embedding (t-SNE), focusing on the role of perplexity. We implemented t-SNE using Scikit-Learn on the Iris dataset, exploring how different perplexity values affect the visualization. Perplexity is a crucial hyperparameter that balances local and global structure in the data, and choosing an appropriate value is key to obtaining meaningful visualizations.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
