{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277f1946",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarchical Clustering with Agglomerative and Divisive Methods Overview\n",
    "\n",
    "This notebook provides an overview of Hierarchical Clustering, focusing on the agglomerative and divisive methods, their working principles, and a basic implementation using a synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c82d50",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### Hierarchical Clustering\n",
    "\n",
    "Hierarchical Clustering is a method of cluster analysis that seeks to build a hierarchy of clusters. It comes in two main forms:\n",
    "- **Agglomerative (Bottom-Up) Clustering**: Starts with each data point as a single cluster and merges the closest pairs of clusters iteratively until all points belong to one cluster.\n",
    "- **Divisive (Top-Down) Clustering**: Starts with all data points in one cluster and recursively splits them into smaller clusters.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Dendrogram**: A tree-like diagram that records the sequences of merges or splits.\n",
    "- **Linkage Criteria**: Determines how the distance between clusters is calculated, with common methods being single, complete, average, and ward linkage.\n",
    "- **Applications**: Hierarchical clustering is widely used in genomics, image analysis, and social network analysis.\n",
    "\n",
    "### Comparison of Agglomerative and Divisive Methods\n",
    "\n",
    "- **Agglomerative Clustering**: More commonly used due to its computational efficiency. It is easier to implement but may suffer from a \"chaining\" effect where clusters can grow in long chains.\n",
    "- **Divisive Clustering**: Conceptually simpler but computationally more intensive. It is less prone to chaining and can sometimes result in more balanced clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d83a1",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Agglomerative Clustering\n",
    "\n",
    "The agglomerative method works as follows:\n",
    "1. **Initialization**: Start with \\( n \\) clusters, each containing a single data point.\n",
    "2. **Merge Step**: At each step, merge the two closest clusters based on a linkage criterion.\n",
    "3. **Repeat**: Continue merging until a single cluster containing all data points is formed.\n",
    "\n",
    "### Divisive Clustering\n",
    "\n",
    "The divisive method works in reverse:\n",
    "1. **Initialization**: Start with a single cluster containing all data points.\n",
    "2. **Split Step**: At each step, split the cluster into two sub-clusters, usually using k-means or similar methods.\n",
    "3. **Repeat**: Continue splitting until each data point is in its own cluster.\n",
    "\n",
    "### Linkage Criteria\n",
    "\n",
    "Linkage criteria determine how the distance between clusters is calculated:\n",
    "- **Single Linkage**: Distance between the closest points in the clusters.\n",
    "- **Complete Linkage**: Distance between the farthest points in the clusters.\n",
    "- **Average Linkage**: Average distance between all points in the clusters.\n",
    "- **Ward Linkage**: Minimizes the variance within clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d32ba",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement both agglomerative and divisive clustering using Scikit-Learn and SciPy on a synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "labels_agg = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Plot Agglomerative Clustering results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels_agg, cmap='viridis')\n",
    "plt.title(\"Agglomerative Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Divisive Clustering (Using Dendrogram)\n",
    "Z = linkage(X, 'ward')\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, truncate_mode='lastp', p=12, show_leaf_counts=False, leaf_rotation=90., leaf_font_size=12., show_contracted=True)\n",
    "plt.title(\"Dendrogram for Divisive Clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1e02d",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an overview of Hierarchical Clustering, focusing on both agglomerative and divisive methods. We implemented these methods using Scikit-Learn and SciPy on a synthetic dataset, demonstrating the principles of clustering and visualizing the results using dendrograms.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
