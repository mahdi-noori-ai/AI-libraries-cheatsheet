{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80121a08",
   "metadata": {},
   "source": [
    "\n",
    "# Principal Component Analysis (PCA) with Whitening Overview\n",
    "\n",
    "This notebook provides an overview of Principal Component Analysis (PCA), focusing on the whitening transformation, its mathematical foundation, and a basic implementation using a dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5d3c2",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms data into a new coordinate system such that the greatest variances by any projection of the data come to lie on the first coordinates (called principal components), the second greatest variances on the second coordinates, and so on.\n",
    "\n",
    "### Whitening in PCA\n",
    "\n",
    "Whitening is a preprocessing step that decorrelates the input data, ensuring that all features have unit variance. This is often done after performing PCA to further transform the data and make it suitable for certain machine learning algorithms. Whitening can reduce redundancy in the dataset and improve model performance.\n",
    "\n",
    "### Applications of PCA and Whitening\n",
    "\n",
    "PCA is widely used for data compression, noise reduction, and visualization. Whitening is particularly useful in neural network training and independent component analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c321bfd",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### PCA\n",
    "\n",
    "Given a dataset \\( X \\) with zero mean, PCA involves the following steps:\n",
    "\n",
    "1. **Covariance Matrix**: Compute the covariance matrix \\( \\Sigma \\):\n",
    "\n",
    "\\[\n",
    "\\Sigma = \\frac{1}{n} X^T X\n",
    "\\]\n",
    "\n",
    "2. **Eigen Decomposition**: Compute the eigenvalues and eigenvectors of \\( \\Sigma \\):\n",
    "\n",
    "\\[\n",
    "\\Sigma v = \\lambda v\n",
    "\\]\n",
    "\n",
    "3. **Principal Components**: The eigenvectors corresponding to the largest eigenvalues are the principal components. Project the data onto these components:\n",
    "\n",
    "\\[\n",
    "Z = X W\n",
    "\\]\n",
    "\n",
    "Where \\( W \\) is the matrix of principal components.\n",
    "\n",
    "### Whitening\n",
    "\n",
    "Whitening involves scaling the principal components such that the resulting features are uncorrelated and have unit variance. This can be achieved by dividing the principal components by the square root of their eigenvalues:\n",
    "\n",
    "\\[\n",
    "Z_{\\text{whitened}} = Z \\Lambda^{-\\frac{1}{2}}\n",
    "\\]\n",
    "\n",
    "Where \\( \\Lambda \\) is the diagonal matrix of eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae5222",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement PCA with and without whitening using Scikit-Learn on the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Apply PCA without whitening\n",
    "pca = PCA(n_components=2, whiten=False)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply PCA with whitening\n",
    "pca_whitened = PCA(n_components=2, whiten=True)\n",
    "X_pca_whitened = pca_whitened.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"PCA without Whitening\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca_whitened[:, 0], X_pca_whitened[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"PCA with Whitening\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2829c4",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an overview of Principal Component Analysis (PCA), focusing on the whitening transformation. We implemented PCA with and without whitening using Scikit-Learn on the Iris dataset, demonstrating the effects of whitening on the transformed data. Whitening helps in making the features uncorrelated and of equal variance, which can be beneficial in certain machine learning tasks.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
