{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2da970ad",
      "metadata": {
        "id": "2da970ad"
      },
      "source": [
        "# Transformers Cheatsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "843d2a5d",
      "metadata": {
        "id": "843d2a5d"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "961bed28",
      "metadata": {
        "id": "961bed28"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from transformers import AutoModelForQuestionAnswering, AutoModelForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f9fc1c",
      "metadata": {
        "id": "21f9fc1c"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0fc16a97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fc16a97",
        "outputId": "3135af05-0851-42c8-9fe7-06d5635abec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Result: [{'label': 'POSITIVE', 'score': 0.9998725652694702}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "# Use the classifier\n",
        "result = classifier(\"Transformers are amazing!\")\n",
        "print(\"Classification Result:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9bfe71a",
      "metadata": {
        "id": "b9bfe71a"
      },
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7a947aa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a947aa0",
        "outputId": "a38bc223-15f7-4597-8b03-49bdb64b24b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Paris\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained model and tokenizer for question answering\n",
        "qa_pipeline = pipeline('question-answering')\n",
        "\n",
        "# Use the pipeline\n",
        "result = qa_pipeline(question=\"What is the capital of France?\", context=\"France's capital is Paris.\")\n",
        "print(\"Answer:\", result['answer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09de0aa1",
      "metadata": {
        "id": "09de0aa1"
      },
      "source": [
        "## Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c8b1c5cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8b1c5cc",
        "outputId": "89c4affb-e82a-4b70-f646-ee99b3a72d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [{'entity': 'I-PER', 'score': 0.9984444, 'index': 4, 'word': 'John', 'start': 11, 'end': 15}, {'entity': 'I-LOC', 'score': 0.9991617, 'index': 9, 'word': 'New', 'start': 30, 'end': 33}, {'entity': 'I-LOC', 'score': 0.9989077, 'index': 10, 'word': 'York', 'start': 34, 'end': 38}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained model and tokenizer for NER\n",
        "ner_pipeline = pipeline('ner')\n",
        "\n",
        "# Use the pipeline\n",
        "result = ner_pipeline(\"My name is John and I live in New York.\")\n",
        "print(\"Named Entities:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b86113",
      "metadata": {
        "id": "45b86113"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5b0daa7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b0daa7a",
        "outputId": "8c26c00b-39e6-4760-ea07-0ef9ed6d2036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: Once upon a time you made a decision to change everything for the better as you took part in a military conflict, what was to become of your past role?\n",
            "\n",
            "I think that it would be a mistake to say that it was the choice to\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained model and tokenizer for text generation\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Use the generator\n",
        "result = generator(\"Once upon a time\", max_length=50, num_return_sequences=1)\n",
        "print(\"Generated Text:\", result[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef5c801",
      "metadata": {
        "id": "8ef5c801"
      },
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d057aef3",
      "metadata": {
        "id": "d057aef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ecaf0f-bad2-40ba-b636-1cc1ee2804bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 142, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  Transformers are very powerful for natural language processing tasks . They have revolutionized the field with their ability to handle long-range dependencies in text . Transformers are more powerful than computers with their long-term dependencies . Transformers can be used to solve complex problems in natural language systems .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained model and tokenizer for summarization\n",
        "summarizer = pipeline('summarization')\n",
        "\n",
        "# Use the summarizer\n",
        "text = \"Transformers are very powerful for natural language processing tasks. They have revolutionized the field with their ability to handle long-range dependencies in text.\"\n",
        "result = summarizer(text)\n",
        "print(\"Summary:\", result[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f8b7a9",
      "metadata": {
        "id": "07f8b7a9"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "653ca48c",
      "metadata": {
        "id": "653ca48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fe05a8-2ae0-4f1a-bc54-cbbe35b5fd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: Les transformateurs sont très puissants pour les tâches NLP.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained model and tokenizer for translation\n",
        "translator = pipeline('translation_en_to_fr')\n",
        "\n",
        "# Use the translator\n",
        "result = translator(\"Transformers are very powerful for NLP tasks.\")\n",
        "print(\"Translation:\", result[0]['translation_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "632ebd94",
      "metadata": {
        "id": "632ebd94"
      },
      "source": [
        "## Custom Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "100c3e33",
      "metadata": {
        "id": "100c3e33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3a14fa-c588-4dc0-e857-be6aa7f1b864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: tensor([[1.2737e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load custom model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(\"Transformers are amazing!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Get predictions\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c215abe",
      "metadata": {
        "id": "7c215abe"
      },
      "source": [
        "## Fine-Tuning a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1ffb76cd",
      "metadata": {
        "id": "1ffb76cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10af60e2-3108-4914-e176-a091cc740c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,  # Replace with your dataset\n",
        "    eval_dataset=None    # Replace with your dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "# trainer.train()  # Uncomment this line to start training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b93b81",
      "metadata": {
        "id": "74b93b81"
      },
      "source": [
        "## Using TensorFlow with Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ab530d22",
      "metadata": {
        "id": "ab530d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9667b8d-4b52-4fb0-c4b8-c76c0c907526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: tf.Tensor([[1.2737003e-04 9.9987257e-01]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "# Load TensorFlow model and tokenizer\n",
        "tf_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tf_model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(\"Transformers are amazing!\", return_tensors=\"tf\")\n",
        "outputs = tf_model(inputs)\n",
        "\n",
        "# Get predictions\n",
        "predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ddb9b99",
      "metadata": {
        "id": "1ddb9b99"
      },
      "source": [
        "## Model for Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141e557c",
      "metadata": {
        "id": "141e557c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model and tokenizer for question answering\n",
        "qa_model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "inputs = tokenizer(\"What is the capital of France?\", \"France's capital is Paris.\", return_tensors=\"pt\")\n",
        "outputs = qa_model(**inputs)\n",
        "\n",
        "# Get the answer\n",
        "answer_start_scores = outputs.start_logits\n",
        "answer_end_scores = outputs.end_logits\n",
        "answer_start = torch.argmax(answer_start_scores)\n",
        "answer_end = torch.argmax(answer_end_scores) + 1\n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "print(\"Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eKgdlfttimX",
      "metadata": {
        "id": "6eKgdlfttimX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}