{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56291294",
   "metadata": {},
   "source": [
    "\n",
    "# Transformers Overview\n",
    "\n",
    "This notebook provides an overview of Transformer models, their architecture, how they work, and a basic implementation using TensorFlow and Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad40d8",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### Transformer Models\n",
    "\n",
    "Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They have since become the foundation for many state-of-the-art models in natural language processing, including BERT, GPT, and T5.\n",
    "\n",
    "### Key Components of Transformers\n",
    "\n",
    "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different words in a sequence relative to each other.\n",
    "- **Positional Encoding**: Adds information about the order of words in a sequence, since the Transformer architecture doesn't inherently capture this.\n",
    "- **Multi-Head Attention**: Enables the model to focus on different parts of the input sequence simultaneously.\n",
    "- **Feedforward Neural Network**: Processes the output of the attention mechanism.\n",
    "- **Layer Normalization**: Stabilizes and accelerates training by normalizing the inputs to each layer.\n",
    "\n",
    "### Applications of Transformers\n",
    "\n",
    "Transformers are used in various NLP tasks, including machine translation, text summarization, sentiment analysis, and more. They are also being adapted for tasks in other domains such as vision and speech processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af8551",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism calculates a weighted sum of the input values, where the weights are determined by the similarity between the input elements.\n",
    "\n",
    "Given an input sequence \\( X \\), the self-attention mechanism involves three steps:\n",
    "\n",
    "1. **Compute Query, Key, and Value Matrices**:\n",
    "\n",
    "\\[\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "\\]\n",
    "\n",
    "2. **Compute Attention Scores**:\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "\\]\n",
    "\n",
    "Where \\( d_k \\) is the dimension of the key vectors.\n",
    "\n",
    "3. **Multi-Head Attention**:\n",
    "\n",
    "\\[\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W_O\n",
    "\\]\n",
    "\n",
    "Where each head \\( \\text{head}_i \\) is a separate attention mechanism.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Positional encoding is added to the input embeddings to inject information about the relative position of words in a sequence:\n",
    "\n",
    "\\[\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "\\]\n",
    "\n",
    "Where \\( pos \\) is the position and \\( i \\) is the dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa2661",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simple Transformer using TensorFlow and Keras on a text classification task (e.g., sentiment analysis with the IMDB dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 500  # Cut texts after this number of words\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Define a simple Transformer block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Positional Embedding Layer\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "# Build the Transformer model\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "maxlen = 500\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = layers.Embedding(max_features, embed_dim)(inputs)\n",
    "pos_encoding = PositionalEncoding(maxlen, embed_dim)(embedding_layer)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)(pos_encoding)\n",
    "pooling = layers.GlobalAveragePooling1D()(transformer_block)\n",
    "dropout = layers.Dropout(0.1)(pooling)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e90e4",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an overview of Transformer models, their architecture, and a basic implementation using the IMDB dataset for sentiment analysis. Transformers have become the state-of-the-art model for many NLP tasks due to their ability to process and learn from sequential data effectively.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
