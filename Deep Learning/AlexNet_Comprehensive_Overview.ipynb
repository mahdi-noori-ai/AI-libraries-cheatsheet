{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a849cb7",
   "metadata": {},
   "source": [
    "\n",
    "# AlexNet: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the AlexNet architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877216",
   "metadata": {},
   "source": [
    "\n",
    "## History of AlexNet\n",
    "\n",
    "AlexNet was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, and was first introduced in 2012. It was the winning model in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) that year, achieving top-5 error rates significantly lower than the previous state of the art. AlexNet's success is often credited with kickstarting the deep learning revolution, as it demonstrated the power of convolutional neural networks (CNNs) on large-scale image classification tasks.\n",
    "\n",
    "The architecture of AlexNet is similar to LeNet but much deeper and larger, designed to handle the complexity of the ImageNet dataset, which contains millions of images across a thousand different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325fc54",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of AlexNet\n",
    "\n",
    "### Architecture\n",
    "\n",
    "AlexNet consists of the following layers:\n",
    "\n",
    "1. **Input Layer**: The input to AlexNet is a 227x227 pixel RGB image.\n",
    "2. **C1 - Convolutional Layer**: Applies 96 convolutional filters of size 11x11 with a stride of 4, resulting in a 55x55 feature map.\n",
    "3. **S2 - Max Pooling Layer**: Applies max pooling with a 3x3 window and a stride of 2, reducing the feature map size to 27x27.\n",
    "4. **C3 - Convolutional Layer**: Applies 256 convolutional filters of size 5x5, resulting in a 27x27 feature map.\n",
    "5. **S4 - Max Pooling Layer**: Similar to S2, reduces the feature map size to 13x13.\n",
    "6. **C5 - Convolutional Layer**: Applies 384 convolutional filters of size 3x3, followed by another convolutional layer with 384 filters and another with 256 filters.\n",
    "7. **S6 - Max Pooling Layer**: Reduces the feature map size to 6x6.\n",
    "8. **F7 - Fully Connected Layer**: Connects all neurons to 4096 neurons.\n",
    "9. **F8 - Fully Connected Layer**: Another fully connected layer with 4096 neurons.\n",
    "10. **Output Layer**: Fully connected layer with 1000 output neurons, one for each class.\n",
    "\n",
    "### ReLU Activation Function\n",
    "\n",
    "AlexNet uses the ReLU activation function, which is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\]\n",
    "\n",
    "This activation function introduces non-linearity into the model and helps in mitigating the vanishing gradient problem, which was prevalent in earlier deep networks.\n",
    "\n",
    "### Dropout Regularization\n",
    "\n",
    "To reduce overfitting, AlexNet employs dropout layers in the fully connected layers. Dropout randomly sets a fraction of the input units to zero at each update during training, which helps prevent the model from becoming too dependent on specific neurons.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "AlexNet uses the cross-entropy loss for classification tasks:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Where \\( y_i \\) is the true label and \\( \\hat{y}_i \\) is the predicted probability.\n",
    "\n",
    "### GPU Utilization\n",
    "\n",
    "One of the key innovations of AlexNet was its use of GPUs to train the model. The model was split across two GPUs, which allowed it to be trained efficiently on the large ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f89d4a",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement the AlexNet architecture using TensorFlow and Keras on a subset of the CIFAR-10 dataset, which contains images from 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bceae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Build the AlexNet model adapted for CIFAR-10\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, (3, 3), strides=1, activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot sample predictions\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "predictions = model.predict(x_test[:10])\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.xlabel(f\"Pred: {class_names[predictions[i].argmax()]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8880f05",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of AlexNet\n",
    "\n",
    "### Advantages\n",
    "- **High Accuracy**: AlexNet demonstrated significantly higher accuracy on the ImageNet dataset than previous models, thanks to its deep architecture and use of ReLU and dropout.\n",
    "- **GPU Utilization**: Pioneered the use of GPUs for training deep networks, which made training large models feasible.\n",
    "- **Modular Architecture**: The architecture of AlexNet has inspired many subsequent models, with its modular structure allowing for easy adjustments and extensions.\n",
    "\n",
    "### Disadvantages\n",
    "- **Resource Intensive**: AlexNet requires significant computational resources for training, including GPUs and substantial memory.\n",
    "- **Overfitting Risk**: Despite dropout regularization, the large number of parameters in AlexNet increases the risk of overfitting, especially on smaller datasets.\n",
    "- **Relatively Large Input Size**: The original AlexNet was designed for 227x227 images, which can be a challenge when adapting to smaller input sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87d6d1",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "AlexNet was a groundbreaking architecture that demonstrated the potential of deep learning in large-scale image classification. Its success in the 2012 ImageNet competition marked a turning point in the field of computer vision and deep learning. Despite its resource intensity and risk of overfitting, AlexNet's innovations, including the use of ReLU, dropout, and GPUs, have had a lasting impact on the development of more advanced models. Understanding AlexNet is crucial for appreciating the evolution of deep learning and its applications in various domains.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
