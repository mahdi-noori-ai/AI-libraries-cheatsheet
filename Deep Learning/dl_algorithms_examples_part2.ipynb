{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df3c98ca",
      "metadata": {
        "id": "df3c98ca"
      },
      "source": [
        "# Deep Learning Algorithms and Their Examples (Part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ff0d78",
      "metadata": {
        "id": "52ff0d78"
      },
      "source": [
        "In this notebook, we will cover more deep learning algorithms along with examples of how to use them. We'll use popular Python libraries such as TensorFlow and Keras to implement these algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb3d786",
      "metadata": {
        "id": "0cb3d786"
      },
      "source": [
        "## 6. Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03f48db",
      "metadata": {
        "id": "b03f48db"
      },
      "source": [
        "Autoencoders are used for unsupervised learning tasks such as anomaly detection and data denoising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8b3af92a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b3af92a",
        "outputId": "6def4cda-5ad4-4d5f-e1cf-283d41badc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6791926622390747\n"
          ]
        }
      ],
      "source": [
        "# Example: Autoencoder using Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "input_dim = 20\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(10, activation='relu')(input_layer)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Sample data\n",
        "X = np.random.random((100, input_dim))\n",
        "\n",
        "# Train the model\n",
        "autoencoder.fit(X, X, epochs=50, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = autoencoder.evaluate(X, X, verbose=0)\n",
        "print(f'Loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3472b5",
      "metadata": {
        "id": "3f3472b5"
      },
      "source": [
        "## 7. Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a951ed0",
      "metadata": {
        "id": "0a951ed0"
      },
      "source": [
        "Transformers are used primarily in natural language processing tasks and are known for their attention mechanisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78f06d82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78f06d82",
        "outputId": "b5ab6ab6-289d-4e7a-8589-77e01858b811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.17261318862438202\n"
          ]
        }
      ],
      "source": [
        "# Example: Transformer using TensorFlow\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, MultiHeadAttention, LayerNormalization, Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "input_layer = Input(shape=(None, 64))\n",
        "attention = MultiHeadAttention(num_heads=4, key_dim=64)(input_layer, input_layer)\n",
        "attention = Dropout(0.1)(attention)\n",
        "attention = LayerNormalization(epsilon=1e-6)(attention)\n",
        "output_layer = Dense(64, activation='relu')(attention)\n",
        "\n",
        "transformer_model = Model(input_layer, output_layer)\n",
        "transformer_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Sample data\n",
        "X = np.random.random((100, 10, 64))\n",
        "y = np.random.random((100, 10, 64))\n",
        "\n",
        "# Train the model\n",
        "transformer_model.fit(X, y, epochs=5, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = transformer_model.evaluate(X, y, verbose=0)\n",
        "print(f'Loss: {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6492d00",
      "metadata": {
        "id": "e6492d00"
      },
      "source": [
        "## 8. Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536faf24",
      "metadata": {
        "id": "536faf24"
      },
      "source": [
        "Attention mechanisms allow models to focus on different parts of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31726afc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31726afc",
        "outputId": "c80922d0-910d-4fad-fa7b-36a6a9efb9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 1.4388\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.1539\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.0778\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0408\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0232\n",
            "Loss: 0.9994813799858093\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, MultiHeadAttention, LayerNormalization, Dense, Dropout\n",
        "\n",
        "# Define the model\n",
        "input_layer = Input(shape=(10, 64))\n",
        "attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(input_layer, input_layer)\n",
        "attention_output = Dropout(0.1)(attention_output)\n",
        "attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "output_layer = Dense(64, activation='relu')(attention_output)\n",
        "\n",
        "# Define the attention model\n",
        "attention_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "attention_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Sample data\n",
        "X = tf.random.normal((100, 10, 64))\n",
        "y = tf.random.normal((100, 10, 64))  # Ensure the shape matches the output of the model\n",
        "\n",
        "# Train the model\n",
        "attention_model.fit(X, y, epochs=5, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = attention_model.evaluate(X, y, verbose=0)\n",
        "print(f'Loss: {loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ad52c0",
      "metadata": {
        "id": "62ad52c0"
      },
      "source": [
        "## 9. Residual Networks (ResNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd16499",
      "metadata": {
        "id": "3cd16499"
      },
      "source": [
        "ResNets are designed to enable training of very deep networks by using skip connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cf14c7f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf14c7f2",
        "outputId": "9fd9ee7b-c9d5-4ef6-f7f9-32f9bd7aa44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - accuracy: 0.0903 - loss: 7.8558\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.3648 - loss: 12.4629\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.6329 - loss: 3.0002\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 189ms/step - accuracy: 0.6343 - loss: 2.4400\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.9559 - loss: 0.1407\n",
            "Loss: 0.894273579120636, Accuracy: 0.8100000023841858\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Flatten, Dense\n",
        "\n",
        "# Define a residual block\n",
        "def residual_block(x, filters, kernel_size=3, stride=1):\n",
        "    shortcut = x\n",
        "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# Define the model\n",
        "input_layer = Input(shape=(32, 32, 3))\n",
        "x = Conv2D(64, kernel_size=7, strides=2, padding='same')(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = residual_block(x, 64)\n",
        "x = residual_block(x, 64)\n",
        "x = Flatten()(x)\n",
        "x = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Define the complete model\n",
        "resnet_model = Model(input_layer, x)\n",
        "resnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Sample data\n",
        "X = tf.random.normal((100, 32, 32, 3))\n",
        "y = tf.random.uniform((100,), maxval=10, dtype=tf.int32)  # Ensure y is integer labels for sparse_categorical_crossentropy\n",
        "\n",
        "# Train the model\n",
        "resnet_model.fit(X, y, epochs=5, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = resnet_model.evaluate(X, y, verbose=0)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f44e6f",
      "metadata": {
        "id": "52f44e6f"
      },
      "source": [
        "## 10. Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eaa487c",
      "metadata": {
        "id": "0eaa487c"
      },
      "source": [
        "Transfer learning involves using a pre-trained model and fine-tuning it on a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef481881",
      "metadata": {
        "id": "ef481881"
      },
      "outputs": [],
      "source": [
        "# Example: Transfer Learning using TensorFlow\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add new layers for fine-tuning\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Define the new model\n",
        "transfer_model = Model(base_model.input, output_layer)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "transfer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Sample data\n",
        "X = np.random.random((100, 224, 224, 3))\n",
        "y = np.random.randint(10, size=(100,))\n",
        "\n",
        "# Train the model\n",
        "transfer_model.fit(X, y, epochs=5, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "loss = transfer_model.evaluate(X, y, verbose=0)\n",
        "print(f'Loss: {loss}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}