{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "322beef4",
      "metadata": {
        "id": "322beef4"
      },
      "source": [
        "\n",
        "# Recurrent Neural Networks (RNN) with Expanded Details\n",
        "\n",
        "This notebook provides an overview of Recurrent Neural Networks (RNN), including their architecture, how they work, implementation on multiple datasets, and hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784d8370",
      "metadata": {
        "id": "784d8370"
      },
      "source": [
        "\n",
        "## Background\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a type of neural network architecture designed to recognize patterns in sequences of data, such as time series or natural language. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, enabling them to maintain a 'memory' of previous inputs.\n",
        "\n",
        "### Key Features of RNNs\n",
        "- **Memory**: RNNs retain information from previous inputs, which is crucial for tasks where context is important.\n",
        "- **Weights Sharing**: The same weights are used across all time steps, making RNNs efficient for sequence processing.\n",
        "- **Applications**: RNNs are widely used in tasks such as language modeling, machine translation, speech recognition, and time series forecasting.\n",
        "\n",
        "### Types of RNNs\n",
        "- **Simple RNN**: The basic form of RNN.\n",
        "- **LSTM (Long Short-Term Memory)**: A more complex variant designed to handle long-term dependencies.\n",
        "- **GRU (Gated Recurrent Unit)**: A variant similar to LSTM but with a simplified architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cfeb91",
      "metadata": {
        "id": "10cfeb91"
      },
      "source": [
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "### The RNN Cell\n",
        "\n",
        "An RNN cell takes an input \\( x_t \\) at time step \\( t \\) and updates its hidden state \\( h_t \\) based on the previous hidden state \\( h_{t-1} \\):\n",
        "\n",
        "\\[\n",
        "h_t = \\tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( W_{xh} \\) and \\( W_{hh} \\) are weight matrices.\n",
        "- \\( b_h \\) is the bias term.\n",
        "- \\( \\tanh \\) is the activation function.\n",
        "\n",
        "The output \\( y_t \\) is typically computed as:\n",
        "\n",
        "\\[\n",
        "y_t = W_{hy}h_t + b_y\n",
        "\\]\n",
        "\n",
        "### LSTM and GRU\n",
        "\n",
        "LSTM and GRU are variants of RNN that include gating mechanisms to better capture long-term dependencies and prevent issues like vanishing gradients.\n",
        "\n",
        "#### LSTM\n",
        "\n",
        "An LSTM cell contains three gates:\n",
        "- **Forget Gate**: Controls what information to discard from the cell state.\n",
        "- **Input Gate**: Controls what information to add to the cell state.\n",
        "- **Output Gate**: Controls what information to output.\n",
        "\n",
        "#### GRU\n",
        "\n",
        "A GRU cell simplifies the LSTM by combining the forget and input gates into a single update gate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1d1864",
      "metadata": {
        "id": "6f1d1864"
      },
      "source": [
        "\n",
        "## Implementation in Python\n",
        "\n",
        "We'll implement RNNs, LSTM, and GRU using TensorFlow and Keras on a text sequence dataset (e.g., IMDB movie reviews).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "31a5b1ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31a5b1ad",
        "outputId": "041e62b7-7e1b-497d-d24e-a9c83fe5987b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 65ms/step - accuracy: 0.5541 - loss: 0.6824 - val_accuracy: 0.7208 - val_loss: 0.5476\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - accuracy: 0.7223 - loss: 0.5457 - val_accuracy: 0.6504 - val_loss: 0.6046\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 54ms/step - accuracy: 0.7982 - loss: 0.4497 - val_accuracy: 0.6810 - val_loss: 0.6020\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 53ms/step - accuracy: 0.7905 - loss: 0.4519 - val_accuracy: 0.6196 - val_loss: 0.6525\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 54ms/step - accuracy: 0.6975 - loss: 0.5644 - val_accuracy: 0.6592 - val_loss: 0.6383\n",
            "SimpleRNN Evaluation:\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.6577 - loss: 0.6408\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.7239 - loss: 0.5364 - val_accuracy: 0.8084 - val_loss: 0.4346\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.8879 - loss: 0.2872 - val_accuracy: 0.8654 - val_loss: 0.3400\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9057 - loss: 0.2391 - val_accuracy: 0.8534 - val_loss: 0.3802\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.9145 - loss: 0.2207 - val_accuracy: 0.8482 - val_loss: 0.4292\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - accuracy: 0.9489 - loss: 0.1402 - val_accuracy: 0.8092 - val_loss: 0.4529\n",
            "LSTM Evaluation:\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.8098 - loss: 0.4552\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step - accuracy: 0.6953 - loss: 0.5482 - val_accuracy: 0.8638 - val_loss: 0.3459\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.8903 - loss: 0.2819 - val_accuracy: 0.8828 - val_loss: 0.2864\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9479 - loss: 0.1511 - val_accuracy: 0.8906 - val_loss: 0.2873\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9768 - loss: 0.0731 - val_accuracy: 0.8868 - val_loss: 0.3732\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.9891 - loss: 0.0367 - val_accuracy: 0.8604 - val_loss: 0.4678\n",
            "GRU Evaluation:\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8542 - loss: 0.4920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4817153811454773, 0.8562800288200378]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
        "\n",
        "# Load the IMDB dataset\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this number of words\n",
        "batch_size = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Define a function to create models\n",
        "def create_model(cell_type='SimpleRNN'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128))\n",
        "    if cell_type == 'SimpleRNN':\n",
        "        model.add(SimpleRNN(128))\n",
        "    elif cell_type == 'LSTM':\n",
        "        model.add(LSTM(128))\n",
        "    elif cell_type == 'GRU':\n",
        "        model.add(GRU(128))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate SimpleRNN\n",
        "simple_rnn_model = create_model('SimpleRNN')\n",
        "simple_rnn_model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)\n",
        "print(\"SimpleRNN Evaluation:\")\n",
        "simple_rnn_model.evaluate(x_test, y_test)\n",
        "\n",
        "# Train and evaluate LSTM\n",
        "lstm_model = create_model('LSTM')\n",
        "lstm_model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)\n",
        "print(\"LSTM Evaluation:\")\n",
        "lstm_model.evaluate(x_test, y_test)\n",
        "\n",
        "# Train and evaluate GRU\n",
        "gru_model = create_model('GRU')\n",
        "gru_model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)\n",
        "print(\"GRU Evaluation:\")\n",
        "gru_model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b488dccd",
      "metadata": {
        "id": "b488dccd"
      },
      "source": [
        "\n",
        "## Hyperparameter Tuning\n",
        "\n",
        "We'll perform hyperparameter tuning using Keras Tuner to find the best values for parameters such as the number of units in the RNN layers, dropout rate, and learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b830c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5b830c5",
        "outputId": "01d3c231-a474-4e4c-9ce1-e97c4bfd23d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras_tuner) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras_tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras_tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "448               |448               |units\n",
            "SimpleRNN         |SimpleRNN         |rnn_type\n",
            "0.001             |0.001             |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 78ms/step - accuracy: 0.5000 - loss: 0.7128 - val_accuracy: 0.5706 - val_loss: 0.6797\n",
            "Epoch 2/2\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 76ms/step - accuracy: 0.5993 - loss: 0.6545 - val_accuracy: 0.6214 - val_loss: 0.6416\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_tuner\n",
        "import keras_tuner as kt\n",
        "\n",
        "def model_builder(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128))\n",
        "\n",
        "    # Tune the number of units in the RNN layers\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "\n",
        "    # Choose between SimpleRNN, LSTM, and GRU\n",
        "    hp_rnn_type = hp.Choice('rnn_type', values=['SimpleRNN', 'LSTM', 'GRU'])\n",
        "\n",
        "    if hp_rnn_type == 'SimpleRNN':\n",
        "        model.add(SimpleRNN(hp_units))\n",
        "    elif hp_rnn_type == 'LSTM':\n",
        "        model.add(LSTM(hp_units))\n",
        "    elif hp_rnn_type == 'GRU':\n",
        "        model.add(GRU(hp_units))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Tune the learning rate for the optimizer\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"The optimal number of units in the RNN layers is {best_hps.get('units')}.\")\n",
        "print(f\"The optimal type of RNN is {best_hps.get('rnn_type')}.\")\n",
        "print(f\"The optimal learning rate is {best_hps.get('learning_rate')}.\")\n",
        "\n",
        "# Build the model with the optimal hyperparameters and train it\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d35f8fbc",
      "metadata": {
        "id": "d35f8fbc"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored Recurrent Neural Networks (RNNs), including their basic architecture, variants like LSTM and GRU, implementation on text data, and hyperparameter tuning. RNNs are a versatile tool for handling sequential data and are widely used in various applications like natural language processing and time series forecasting.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}