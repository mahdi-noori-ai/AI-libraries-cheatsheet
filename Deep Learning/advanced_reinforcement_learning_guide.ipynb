{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ed1d7e4",
      "metadata": {
        "id": "4ed1d7e4"
      },
      "source": [
        "# Reinforcement Learning Guide - Continuation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158652f7",
      "metadata": {
        "id": "158652f7"
      },
      "source": [
        "In this continuation, we will explore more advanced Reinforcement Learning algorithms and techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4603c97",
      "metadata": {
        "id": "f4603c97"
      },
      "source": [
        "## 1. Policy Gradient Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c14572",
      "metadata": {
        "id": "91c14572"
      },
      "source": [
        "Policy Gradient methods are a type of Reinforcement Learning algorithm that directly parameterizes the policy and optimizes the parameters using gradient ascent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca45d440",
      "metadata": {
        "id": "ca45d440"
      },
      "source": [
        "### REINFORCE Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff45e9b",
      "metadata": {
        "id": "bff45e9b"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Set parameters\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Build the policy network\n",
        "policy_network = Sequential([\n",
        "    Dense(24, input_dim=state_size, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(action_size, activation='softmax')\n",
        "])\n",
        "policy_network.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy')\n",
        "\n",
        "def choose_action(state):\n",
        "    state = state.reshape([1, state_size])\n",
        "    prob = policy_network.predict(state)[0]\n",
        "    return np.random.choice(action_size, p=prob)\n",
        "\n",
        "def discount_rewards(rewards, gamma=0.99):\n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    cumulative_rewards = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        cumulative_rewards = cumulative_rewards * gamma + rewards[t]\n",
        "        discounted_rewards[t] = cumulative_rewards\n",
        "    return discounted_rewards\n",
        "\n",
        "# Training parameters\n",
        "n_episodes = 1000\n",
        "gamma = 0.99\n",
        "\n",
        "# Training loop\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    states, actions, rewards = [], [], []\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            discounted_rewards = discount_rewards(rewards, gamma)\n",
        "            discounted_rewards -= np.mean(discounted_rewards)\n",
        "            discounted_rewards /= np.std(discounted_rewards)\n",
        "\n",
        "            states = np.vstack(states)\n",
        "            actions = np.array(actions)\n",
        "            advantages = discounted_rewards\n",
        "\n",
        "            actions_one_hot = np.zeros([len(actions), action_size])\n",
        "            actions_one_hot[np.arange(len(actions)), actions] = 1\n",
        "\n",
        "            policy_network.fit(states, actions_one_hot, sample_weight=advantages, verbose=0)\n",
        "\n",
        "            print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Test the agent\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    state = state.reshape([1, state_size])\n",
        "    action = np.argmax(policy_network.predict(state)[0])\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "print(\"Total reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3b3af8",
      "metadata": {
        "id": "1d3b3af8"
      },
      "source": [
        "## 2. Actor-Critic Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da1a95a7",
      "metadata": {
        "id": "da1a95a7"
      },
      "source": [
        "Actor-Critic methods combine policy-based and value-based methods. The actor updates the policy, and the critic estimates the value function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee0ef82",
      "metadata": {
        "id": "7ee0ef82"
      },
      "source": [
        "### A2C (Advantage Actor-Critic) Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa713128",
      "metadata": {
        "id": "aa713128"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Set parameters\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Build the actor network\n",
        "actor_network = Sequential([\n",
        "    Dense(24, input_dim=state_size, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(action_size, activation='softmax')\n",
        "])\n",
        "actor_network.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy')\n",
        "\n",
        "# Build the critic network\n",
        "critic_network = Sequential([\n",
        "    Dense(24, input_dim=state_size, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "critic_network.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "def choose_action(state):\n",
        "    state = state.reshape([1, state_size])\n",
        "    prob = actor_network.predict(state)[0]\n",
        "    return np.random.choice(action_size, p=prob)\n",
        "\n",
        "# Training parameters\n",
        "n_episodes = 1000\n",
        "gamma = 0.99\n",
        "\n",
        "# Training loop\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    states, actions, rewards, values = [], [], [], []\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        values.append(critic_network.predict(state)[0][0])\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            values.append(critic_network.predict(next_state.reshape([1, state_size]))[0][0])\n",
        "            advantages, returns = [], []\n",
        "            gae = 0\n",
        "            for t in reversed(range(len(rewards))):\n",
        "                delta = rewards[t] + gamma * values[t+1] - values[t]\n",
        "                gae = delta + gamma * gae\n",
        "                advantages.append(gae)\n",
        "                returns.append(gae + values[t])\n",
        "            advantages.reverse()\n",
        "            returns.reverse()\n",
        "\n",
        "            states = np.vstack(states)\n",
        "            actions = np.array(actions)\n",
        "            advantages = np.array(advantages)\n",
        "            returns = np.array(returns)\n",
        "\n",
        "            actions_one_hot = np.zeros([len(actions), action_size])\n",
        "            actions_one_hot[np.arange(len(actions)), actions] = 1\n",
        "\n",
        "            actor_network.fit(states, actions_one_hot, sample_weight=advantages, verbose=0)\n",
        "            critic_network.fit(states, returns, verbose=0)\n",
        "\n",
        "            print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Test the agent\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    state = state.reshape([1, state_size])\n",
        "    action = np.argmax(actor_network.predict(state)[0])\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "print(\"Total reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Y5ZqN4-RMDK"
      },
      "id": "3Y5ZqN4-RMDK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}