{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c663aa98",
   "metadata": {},
   "source": [
    "\n",
    "# Long Short-Term Memory (LSTM) Networks Overview\n",
    "\n",
    "This notebook provides an overview of Long Short-Term Memory (LSTM) networks, their architecture, how they work, and a basic implementation using a text dataset (IMDB reviews).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7581f",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "### Long Short-Term Memory (LSTM) Networks\n",
    "\n",
    "LSTM networks are a type of recurrent neural network (RNN) designed to overcome the limitations of standard RNNs, particularly the vanishing gradient problem. They are well-suited for tasks that involve sequential data, such as language modeling, time series prediction, and speech recognition.\n",
    "\n",
    "### Key Components of LSTMs\n",
    "\n",
    "- **Cell State**: A memory unit that carries information across time steps.\n",
    "- **Gates**: Mechanisms that regulate the flow of information:\n",
    "  - **Forget Gate**: Decides what information to discard.\n",
    "  - **Input Gate**: Decides what new information to store.\n",
    "  - **Output Gate**: Decides what information to output.\n",
    "\n",
    "### Applications of LSTMs\n",
    "\n",
    "LSTMs are widely used in tasks such as text generation, sentiment analysis, machine translation, and speech synthesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ec6cc",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The LSTM Cell\n",
    "\n",
    "An LSTM cell updates its hidden state \\( h_t \\) and cell state \\( C_t \\) using the following steps:\n",
    "\n",
    "1. **Forget Gate** \\( f_t \\):\n",
    "\n",
    "\\[\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)\n",
    "\\]\n",
    "\n",
    "2. **Input Gate** \\( i_t \\) and **Candidate Values** \\( \\tilde{C}_t \\):\n",
    "\n",
    "\\[\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)\n",
    "\\]\n",
    "\\[\n",
    "\\tilde{C}_t = \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\n",
    "\\]\n",
    "\n",
    "3. **Update Cell State** \\( C_t \\):\n",
    "\n",
    "\\[\n",
    "C_t = f_t \\ast C_{t-1} + i_t \\ast \\tilde{C}_t\n",
    "\\]\n",
    "\n",
    "4. **Output Gate** \\( o_t \\) and **Hidden State** \\( h_t \\):\n",
    "\n",
    "\\[\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o)\n",
    "\\]\n",
    "\\[\n",
    "h_t = o_t \\ast \\tanh(C_t)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\sigma \\) is the sigmoid activation function.\n",
    "- \\( \\ast \\) denotes element-wise multiplication.\n",
    "- \\( W \\) and \\( b \\) are the weight matrices and bias vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8873195",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement an LSTM using TensorFlow and Keras on a text sequence dataset (IMDB movie reviews).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0766c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 500  # Cut texts after this number of words\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.imdb.load_data(num_words=max_features)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(max_features, 128, input_length=maxlen),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Plot sample predictions\n",
    "predictions = model.predict(x_test[:10])\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}, Actual: {'Positive' if y_test[i] == 1 else 'Negative'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fce69",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an overview of LSTM networks, their architecture, and a basic implementation using the IMDB dataset. LSTMs are a powerful tool for sequential data processing, capable of learning long-term dependencies.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
