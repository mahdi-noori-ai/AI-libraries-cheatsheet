{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27f170a",
   "metadata": {},
   "source": [
    "\n",
    "# ResNet: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the ResNet architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e31dc2",
   "metadata": {},
   "source": [
    "\n",
    "## History of ResNet\n",
    "\n",
    "ResNet, short for Residual Networks, was introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 paper \"Deep Residual Learning for Image Recognition.\" The model was a groundbreaking advancement in deep learning, winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2015.\n",
    "\n",
    "The key innovation of ResNet is the introduction of \"skip connections\" or \"residual connections,\" which help mitigate the vanishing gradient problem that commonly occurs when training deep neural networks. By allowing the model to learn residual mappings instead of direct mappings, ResNet enabled the training of much deeper networks than was previously possible, with ResNet-50, ResNet-101, and ResNet-152 being notable examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b946e6",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of ResNet\n",
    "\n",
    "### Architecture\n",
    "\n",
    "ResNet's architecture is built around residual blocks, which allow the network to learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. This is achieved by adding a shortcut connection that skips one or more layers.\n",
    "\n",
    "A typical residual block includes:\n",
    "- **Identity Shortcut Connection**: This is a direct connection that bypasses one or more layers and adds the input directly to the output of a deeper layer.\n",
    "- **Residual Mapping**: Instead of directly learning the desired underlying mapping, \\( H(x) \\), the network learns the residual mapping \\( F(x) = H(x) - x \\). The original function thus becomes \\( H(x) = F(x) + x \\).\n",
    "\n",
    "The overall architecture of ResNet is a stack of these residual blocks, followed by a global average pooling layer and a fully connected layer with a softmax activation function to produce class probabilities.\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "The skip connection in a residual block can be mathematically expressed as:\n",
    "\n",
    "\\[\n",
    "y = F(x, \\{W_i\\}) + x\n",
    "\\]\n",
    "\n",
    "Where \\( y \\) is the output, \\( F(x, \\{W_i\\}) \\) represents the residual function, and \\( x \\) is the input to the residual block. The network thus learns the residual mapping \\( F(x) \\) rather than the original mapping \\( H(x) \\).\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "ResNet uses the cross-entropy loss for classification tasks:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Where \\( y_i \\) is the true label and \\( \\hat{y}_i \\) is the predicted probability.\n",
    "\n",
    "### ReLU Activation Function\n",
    "\n",
    "ResNet uses the ReLU activation function, which is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\]\n",
    "\n",
    "This introduces non-linearity into the network, allowing it to model complex patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8729a",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simplified version of ResNet using TensorFlow and Keras on the CIFAR-10 dataset, which contains images from 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define the residual block\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True):\n",
    "    if conv_shortcut:\n",
    "        shortcut = layers.Conv2D(4 * filters, 1, strides=stride)(x)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, 1, strides=stride)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='SAME')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(4 * filters, 1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.add([shortcut, x])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# Build a simplified ResNet model for CIFAR-10\n",
    "input_layer = layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "x = layers.Conv2D(64, 3, strides=1, padding='same')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = residual_block(x, 64)\n",
    "x = residual_block(x, 64, conv_shortcut=False)\n",
    "\n",
    "x = residual_block(x, 128, stride=2)\n",
    "x = residual_block(x, 128, conv_shortcut=False)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(input_layer, x)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot sample predictions\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "predictions = model.predict(x_test[:10])\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.xlabel(f\"Pred: {class_names[predictions[i].argmax()]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc7f90",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of ResNet\n",
    "\n",
    "### Advantages\n",
    "- **Mitigates Vanishing Gradient Problem**: The introduction of residual connections helps prevent the vanishing gradient problem, allowing for much deeper networks.\n",
    "- **High Accuracy**: ResNet achieved state-of-the-art accuracy on the ImageNet dataset, demonstrating the effectiveness of deep residual learning.\n",
    "- **Scalability**: ResNet's architecture scales well, with variants like ResNet-50, ResNet-101, and ResNet-152 being commonly used.\n",
    "\n",
    "### Disadvantages\n",
    "- **Complexity**: The architecture of ResNet is more complex than earlier models, making it more challenging to implement and understand.\n",
    "- **Training Time**: Due to its depth, ResNet models can take longer to train, requiring careful tuning of hyperparameters and training strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e4b6e",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "ResNet was a significant advancement in deep learning architecture, introducing residual learning, which allowed for the training of much deeper networks without suffering from the vanishing gradient problem. Its success in the 2015 ImageNet competition demonstrated the power of deep residual networks, influencing the design of subsequent models. ResNet remains a key architecture in the evolution of CNNs, with its principles continuing to shape modern deep learning models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
