{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63552ef7",
   "metadata": {},
   "source": [
    "\n",
    "# Gated Recurrent Units (GRUs) with Expanded Details\n",
    "\n",
    "This notebook provides an overview of Gated Recurrent Units (GRUs), including their architecture, how they work, implementation on a dataset, and hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c8217",
   "metadata": {},
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem in standard RNNs. GRUs are a simpler alternative to Long Short-Term Memory (LSTM) networks, as they have fewer parameters and are faster to train.\n",
    "\n",
    "### Key Features of GRUs\n",
    "- **Gating Mechanisms**: GRUs use update and reset gates to control the flow of information.\n",
    "- **Simplified Architecture**: Compared to LSTMs, GRUs have fewer parameters, making them computationally more efficient.\n",
    "- **Applications**: GRUs are used in tasks like language modeling, speech recognition, and time series prediction.\n",
    "\n",
    "### GRU Cell\n",
    "A GRU cell has two main components:\n",
    "- **Update Gate**: Determines how much of the past information to keep.\n",
    "- **Reset Gate**: Determines how much of the past information to forget.\n",
    "\n",
    "The GRU architecture combines the hidden state and the cell state into a single vector, simplifying the model and making it more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b72f53",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### The GRU Cell\n",
    "\n",
    "A GRU cell updates its hidden state \\( h_t \\) based on the previous hidden state \\( h_{t-1} \\) and the current input \\( x_t \\):\n",
    "\n",
    "1. **Reset Gate** \\( r_t \\):\n",
    "\n",
    "\\[\n",
    "r_t = \\sigma(W_{xr}x_t + W_{hr}h_{t-1} + b_r)\n",
    "\\]\n",
    "\n",
    "2. **Update Gate** \\( z_t \\):\n",
    "\n",
    "\\[\n",
    "z_t = \\sigma(W_{xz}x_t + W_{hz}h_{t-1} + b_z)\n",
    "\\]\n",
    "\n",
    "3. **Candidate Hidden State** \\( \\tilde{h}_t \\):\n",
    "\n",
    "\\[\n",
    "\\tilde{h}_t = \\tanh(W_{xh}x_t + r_t \\ast (W_{hh}h_{t-1}) + b_h)\n",
    "\\]\n",
    "\n",
    "4. **Final Hidden State** \\( h_t \\):\n",
    "\n",
    "\\[\n",
    "h_t = (1 - z_t) \\ast h_{t-1} + z_t \\ast \\tilde{h}_t\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\sigma \\) is the sigmoid activation function.\n",
    "- \\( \\ast \\) denotes element-wise multiplication.\n",
    "- \\( W \\) and \\( b \\) are the weight matrices and bias vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d569c",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a GRU using TensorFlow and Keras on a text sequence dataset (e.g., IMDB movie reviews).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce397ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# Load the IMDB dataset\n",
    "max_features = 10000  # Number of words to consider as features\n",
    "maxlen = 500  # Cut texts after this number of words\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the GRU model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"GRU Evaluation:\")\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e2aa0",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "We'll perform hyperparameter tuning using Keras Tuner to find the best values for parameters such as the number of units in the GRU layer, dropout rate, and learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58100ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    \n",
    "    # Tune the number of units in the GRU layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    \n",
    "    model.add(GRU(hp_units))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='gru_tuning')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"The optimal number of units in the GRU layer is {best_hps.get('units')}.\")\n",
    "print(f\"The optimal learning rate is {best_hps.get('learning_rate')}.\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e93d85",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored Gated Recurrent Units (GRUs), including their basic architecture, implementation on text data, and hyperparameter tuning. GRUs are a powerful tool for handling sequential data, particularly when efficiency is a concern compared to LSTMs.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
