{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f56ca1c",
   "metadata": {},
   "source": [
    "\n",
    "# Bidirectional RNN: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Bidirectional Recurrent Neural Network (RNN) architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3dfba",
   "metadata": {},
   "source": [
    "\n",
    "## History of Bidirectional RNN\n",
    "\n",
    "Bidirectional Recurrent Neural Networks (BRNNs) were introduced by Mike Schuster and Kuldip K. Paliwal in their 1997 paper \"Bidirectional Recurrent Neural Networks.\" The main idea behind BRNNs is to use two RNNs, one processing the input sequence from start to end and the other from end to start. This allows the network to capture information from both past and future contexts, making it particularly effective for tasks where context in both directions is important, such as speech recognition and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b6593",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Bidirectional RNN\n",
    "\n",
    "### Architecture\n",
    "\n",
    "In a Bidirectional RNN, two separate RNNs are used: a forward RNN that processes the input sequence in its natural order (from \\( t_1 \\) to \\( t_n \\)) and a backward RNN that processes the input sequence in reverse order (from \\( t_n \\) to \\( t_1 \\)). The hidden states from both RNNs are then combined, usually by concatenation, to produce the final output.\n",
    "\n",
    "Let \\( \\overrightarrow{h_t} \\) be the hidden state of the forward RNN at time step \\( t \\) and \\( \\overleftarrow{h_t} \\) be the hidden state of the backward RNN at time step \\( t \\). The output \\( y_t \\) at time step \\( t \\) is computed as:\n",
    "\n",
    "\\[\n",
    "y_t = W_{hy}[\\overrightarrow{h_t}, \\overleftarrow{h_t}] + b_y\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( [\\overrightarrow{h_t}, \\overleftarrow{h_t}] \\) denotes the concatenation of the forward and backward hidden states.\n",
    "- \\( W_{hy} \\) is the weight matrix for the output layer.\n",
    "- \\( b_y \\) is the bias term.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Bidirectional RNNs typically use the same loss functions as unidirectional RNNs, depending on the task. For sequence classification, cross-entropy loss is commonly used:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Where \\( y_i \\) is the true label, and \\( \\hat{y}_i \\) is the predicted output.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a Bidirectional RNN involves backpropagation through time (BPTT) for both the forward and backward RNNs. The gradients from both directions are combined to update the model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a6e8e",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a Bidirectional RNN using TensorFlow and Keras on the IMDB sentiment analysis dataset, which involves predicting the sentiment of movie reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Define the Bidirectional RNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.SimpleRNN(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e4a0a",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Bidirectional RNN\n",
    "\n",
    "### Advantages\n",
    "- **Contextual Understanding**: Bidirectional RNNs capture information from both past and future contexts, making them highly effective for tasks like speech recognition, where context is crucial.\n",
    "- **Improved Accuracy**: They often outperform unidirectional RNNs on tasks where future context is as important as past context.\n",
    "\n",
    "### Disadvantages\n",
    "- **Increased Computational Cost**: Bidirectional RNNs require more memory and computational resources due to the two RNNs running in parallel.\n",
    "- **Complexity**: They are more complex to train and tune compared to unidirectional RNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22782ee0",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Bidirectional RNNs represent an important advancement in the field of sequential data processing, allowing models to consider context from both directions. While they are more computationally expensive, their ability to capture comprehensive context makes them invaluable for tasks like natural language processing and speech recognition.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
