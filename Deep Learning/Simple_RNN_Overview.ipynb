{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2a27f2",
   "metadata": {},
   "source": [
    "\n",
    "# Simple RNN: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Simple Recurrent Neural Network (RNN) architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b975a1b",
   "metadata": {},
   "source": [
    "\n",
    "## History of Simple RNN\n",
    "\n",
    "The Simple Recurrent Neural Network (RNN) was one of the earliest types of artificial neural networks designed to handle sequential data. Introduced in the 1980s, RNNs were developed to process sequences by maintaining a hidden state that captures information about previous inputs. However, Simple RNNs suffer from issues such as vanishing and exploding gradients, which limit their effectiveness for long sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0626c54",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Simple RNN\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A Simple RNN is composed of a loop that allows information to be passed from one step to the next. The hidden state \\( h_t \\) at time step \\( t \\) is calculated as follows:\n",
    "\n",
    "\\[\n",
    "h_t = \\tanh(W_{hx}x_t + W_{hh}h_{t-1} + b_h)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( x_t \\) is the input at time step \\( t \\).\n",
    "- \\( W_{hx} \\) and \\( W_{hh} \\) are weight matrices.\n",
    "- \\( h_{t-1} \\) is the hidden state from the previous time step.\n",
    "- \\( b_h \\) is a bias term.\n",
    "- \\( \\tanh \\) is the activation function.\n",
    "\n",
    "The output \\( y_t \\) is then computed by:\n",
    "\n",
    "\\[\n",
    "y_t = W_{hy}h_t + b_y\n",
    "\\]\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "For sequence prediction tasks, Simple RNNs typically use the mean squared error or cross-entropy loss, depending on the output type:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "or\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "\\]\n",
    "\n",
    "Where \\( y_i \\) is the true label, and \\( \\hat{y}_i \\) is the predicted output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4e65d",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a Simple RNN using TensorFlow and Keras on a sequential dataset like the IMDB sentiment analysis dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac23021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Load and preprocess the IMDB dataset\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Define the Simple RNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.SimpleRNN(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3315e37",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Simple RNN\n",
    "\n",
    "### Advantages\n",
    "- **Simplicity**: Simple RNNs are easy to understand and implement, making them suitable for small-scale tasks.\n",
    "- **Sequential Data Handling**: They can process sequences of varying lengths, making them useful for tasks like time series prediction and natural language processing.\n",
    "\n",
    "### Disadvantages\n",
    "- **Vanishing/Exploding Gradients**: Simple RNNs struggle with long sequences due to vanishing or exploding gradients during backpropagation.\n",
    "- **Limited Memory**: They have limited ability to retain information over long sequences, which affects their performance on complex tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dffc8f0",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Simple RNNs laid the groundwork for more advanced recurrent architectures like LSTM and GRU. While they have limitations, particularly with long sequences, their simplicity and effectiveness in certain tasks make them a valuable tool in the deep learning toolkit. Understanding Simple RNNs is crucial for grasping the evolution of sequential models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
