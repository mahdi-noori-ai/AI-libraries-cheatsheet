{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ea6c46",
   "metadata": {},
   "source": [
    "\n",
    "# Encoder-Decoder RNN: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Encoder-Decoder Recurrent Neural Network (RNN) architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee112b",
   "metadata": {},
   "source": [
    "\n",
    "## History of Encoder-Decoder RNN\n",
    "\n",
    "The Encoder-Decoder architecture was introduced in the early 2010s and became widely known with the work of Sutskever, Vinyals, and Le in 2014, where it was applied to machine translation tasks. The Encoder-Decoder architecture was designed to handle sequences of varying lengths, making it a fundamental building block for tasks like machine translation, summarization, and question-answering. The model uses one RNN (the encoder) to process the input sequence into a fixed-length context vector, which is then decoded by another RNN (the decoder) into the target sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2f704",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Encoder-Decoder RNN\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The Encoder-Decoder architecture consists of two main components:\n",
    "\n",
    "1. **Encoder**: The encoder RNN processes the input sequence \\( (x_1, x_2, \\dots, x_T) \\) and compresses the information into a fixed-length context vector \\( c \\). The context vector is the final hidden state of the encoder.\n",
    "\n",
    "\\[\n",
    "h_t = \\text{RNN}(x_t, h_{t-1})\n",
    "\\]\n",
    "\\[\n",
    "c = h_T\n",
    "\\]\n",
    "\n",
    "2. **Decoder**: The decoder RNN generates the output sequence \\( (y_1, y_2, \\dots, y_T') \\) based on the context vector \\( c \\) and its own previous hidden states.\n",
    "\n",
    "\\[\n",
    "s_t = \\text{RNN}(y_{t-1}, s_{t-1}, c)\n",
    "\\]\n",
    "\\[\n",
    "\\hat{y}_t = \\text{Softmax}(W_s s_t + b_s)\n",
    "\\]\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "For sequence-to-sequence tasks, the Encoder-Decoder RNN typically uses cross-entropy loss:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = -\\sum_{t=1}^{T'} \\sum_{k=1}^{K} y_t^{(k)} \\log(\\hat{y}_t^{(k)})\n",
    "\\]\n",
    "\n",
    "Where \\( y_t^{(k)} \\) is the true label at time step \\( t \\) for class \\( k \\), and \\( \\hat{y}_t^{(k)} \\) is the predicted probability for class \\( k \\) at time step \\( t \\).\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "In more advanced Encoder-Decoder architectures, an attention mechanism is often used to allow the decoder to focus on different parts of the input sequence at each time step, rather than relying solely on a fixed context vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae517573",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simplified Encoder-Decoder RNN using TensorFlow and Keras on a small-scale sequence-to-sequence task, such as translating a sequence of numbers into their spelled-out forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Preparation\n",
    "def generate_data(num_samples=10000, max_len=5):\n",
    "    x = np.random.randint(1, 10, size=(num_samples, max_len))\n",
    "    y = np.array([[str(digit) for digit in seq] for seq in x])\n",
    "    y = np.array([' '.join(seq) for seq in y])\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = generate_data()\n",
    "x_test, y_test = generate_data(1000)\n",
    "\n",
    "# Define the Encoder-Decoder Model\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=(None,))\n",
    "encoder = layers.Embedding(input_dim=10, output_dim=latent_dim)(encoder_inputs)\n",
    "encoder_outputs, state_h = layers.SimpleRNN(latent_dim, return_state=True)(encoder)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "decoder = layers.Embedding(input_dim=10, output_dim=latent_dim)(decoder_inputs)\n",
    "decoder_rnn = layers.SimpleRNN(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_rnn(decoder, initial_state=state_h)\n",
    "decoder_dense = layers.Dense(10, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Build the model\n",
    "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31696185",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Encoder-Decoder RNN\n",
    "\n",
    "### Advantages\n",
    "- **Flexible Sequence Handling**: The Encoder-Decoder architecture can handle sequences of varying lengths, making it suitable for tasks like translation and summarization.\n",
    "- **Foundation for Attention Mechanisms**: The architecture serves as a basis for more advanced models that incorporate attention mechanisms.\n",
    "\n",
    "### Disadvantages\n",
    "- **Bottleneck Problem**: The fixed-length context vector can become a bottleneck, limiting the model's ability to handle long sequences effectively.\n",
    "- **Increased Complexity**: The architecture is more complex to implement and requires careful tuning, especially when used with attention mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf6f7e",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "The Encoder-Decoder RNN architecture is a powerful model for sequence-to-sequence tasks, providing the foundation for many state-of-the-art models in machine translation, summarization, and more. While it has limitations, such as the bottleneck problem, its flexibility and adaptability make it a cornerstone of modern deep learning for sequential data.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
