{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aea6f73",
   "metadata": {},
   "source": [
    "\n",
    "# Vanilla Autoencoder: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Vanilla Autoencoder architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc0021",
   "metadata": {},
   "source": [
    "\n",
    "## History of Vanilla Autoencoder\n",
    "\n",
    "The concept of autoencoders dates back to the 1980s, and they have since become fundamental tools in unsupervised learning. A Vanilla Autoencoder is the simplest form of an autoencoder, consisting of an encoder and a decoder. The autoencoder was initially designed for dimensionality reduction, feature learning, and later found applications in data denoising and generative modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996ee08",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Vanilla Autoencoder\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A Vanilla Autoencoder consists of two main components:\n",
    "\n",
    "1. **Encoder**: The encoder compresses the input data \\( x \\) into a lower-dimensional representation \\( z \\), often called the latent space.\n",
    "\n",
    "\\[\n",
    "z = f(x) = \\sigma(Wx + b)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( W \\) is the weight matrix.\n",
    "- \\( b \\) is the bias vector.\n",
    "- \\( \\sigma \\) is the activation function.\n",
    "\n",
    "2. **Decoder**: The decoder reconstructs the input data from the latent representation \\( z \\).\n",
    "\n",
    "\\[\n",
    "\\hat{x} = g(z) = \\sigma'(W'z + b')\n",
    "\\]\n",
    "\n",
    "Where \\( W' \\), \\( b' \\), and \\( \\sigma' \\) are the weights, biases, and activation function for the decoder.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The objective of a Vanilla Autoencoder is to minimize the reconstruction error, typically measured using mean squared error:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
    "\\]\n",
    "\n",
    "Where \\( x_i \\) is the original input and \\( \\hat{x}_i \\) is the reconstructed input.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a Vanilla Autoencoder involves backpropagation to minimize the reconstruction loss, updating the weights of both the encoder and decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3130c1",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a Vanilla Autoencoder using TensorFlow and Keras on the MNIST dataset, which consists of handwritten digit images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), 28, 28, 1))\n",
    "x_test = x_test.reshape((len(x_test), 28, 28, 1))\n",
    "\n",
    "# Define the Vanilla Autoencoder model\n",
    "input_img = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "encoded = layers.Flatten()(input_img)\n",
    "encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(28 * 28, activation='sigmoid')(decoded)\n",
    "decoded = layers.Reshape((28, 28, 1))(decoded)\n",
    "\n",
    "# Build the model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, validation_data=(x_test, x_test))\n",
    "\n",
    "# Evaluate the model\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Plot original and reconstructed images\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ade66",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Vanilla Autoencoder\n",
    "\n",
    "### Advantages\n",
    "- **Dimensionality Reduction**: Vanilla Autoencoders can effectively reduce the dimensionality of data, capturing essential features.\n",
    "- **Unsupervised Learning**: They can learn representations without labeled data, making them useful for unsupervised learning tasks.\n",
    "\n",
    "### Disadvantages\n",
    "- **Limited Complexity**: Vanilla Autoencoders may struggle with complex data and often require deeper or more complex architectures.\n",
    "- **Overfitting**: They can easily overfit, especially when the latent space is too large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c31cb",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Vanilla Autoencoders are foundational models in deep learning, providing a simple yet powerful framework for tasks like dimensionality reduction, feature learning, and data denoising. While they have limitations, their simplicity and effectiveness in various applications make them a valuable tool in the deep learning toolkit.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
