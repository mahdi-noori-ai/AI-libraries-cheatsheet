{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5725caa",
   "metadata": {},
   "source": [
    "# Most Used Functions in Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adaeea6",
   "metadata": {},
   "source": [
    "Data pipelines are essential for automating the flow of data from source to destination, while performing necessary transformations along the way. Here, we will cover some of the most commonly used functions and techniques in data pipelines using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cee34c",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0f3bd",
   "metadata": {},
   "source": [
    "Data extraction involves retrieving data from various sources such as databases, APIs, and files. This is the first step in any data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracting data from a SQL database using SQLAlchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Creating an engine and extracting data from a SQL database\n",
    "engine = create_engine('sqlite:///example.db')\n",
    "data = pd.read_sql('SELECT * FROM table_name', engine)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dca87d",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef5957",
   "metadata": {},
   "source": [
    "Data cleaning involves removing or fixing incorrect, corrupted, or incomplete data. It is crucial for ensuring data quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e982f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data cleaning using pandas\n",
    "# Removing duplicate rows\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Filling missing values\n",
    "data['column'].fillna(value='default_value', inplace=True)\n",
    "\n",
    "# Removing rows with missing values\n",
    "clean_data = data.dropna()\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a03c2",
   "metadata": {},
   "source": [
    "## 3. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ca566",
   "metadata": {},
   "source": [
    "Data transformation includes operations such as aggregations, filtering, and converting data types. This step prepares data for analysis or storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ec6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data transformation using pandas\n",
    "# Aggregating data\n",
    "aggregated_data = data.groupby('column').sum()\n",
    "\n",
    "# Converting data types\n",
    "data['column'] = data['column'].astype(float)\n",
    "\n",
    "# Filtering data\n",
    "filtered_data = data[data['column'] > threshold]\n",
    "print(filtered_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93011f9",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81dc47d",
   "metadata": {},
   "source": [
    "Data loading is the process of writing data to a destination, such as a database, data warehouse, or file. This is the final step in a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading data into a CSV file using pandas\n",
    "# Saving data to a CSV file\n",
    "data.to_csv('clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61ac10",
   "metadata": {},
   "source": [
    "## 5. Orchestrating Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8044c01",
   "metadata": {},
   "source": [
    "Orchestration involves scheduling and managing the execution of data pipeline tasks. Tools like Apache Airflow and Prefect are commonly used for orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple data pipeline using Prefect\n",
    "from prefect import task, Flow\n",
    "\n",
    "@task\n",
    "def extract():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "@task\n",
    "def transform(data):\n",
    "    return [x * 10 for x in data]\n",
    "\n",
    "@task\n",
    "def load(data):\n",
    "    print(f'Data: {data}')\n",
    "\n",
    "with Flow('ETL') as flow:\n",
    "    data = extract()\n",
    "    transformed_data = transform(data)\n",
    "    load(transformed_data)\n",
    "\n",
    "flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cddb470",
   "metadata": {},
   "source": [
    "## 6. Monitoring and Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b562d78",
   "metadata": {},
   "source": [
    "Monitoring and logging are essential for tracking the performance and success of data pipelines. They help in identifying issues and ensuring that the pipelines are running smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple logging using Python's logging module\n",
    "import logging\n",
    "\n",
    "# Configuring logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Adding logs in a data pipeline task\n",
    "def pipeline_task():\n",
    "    logging.info('Task started')\n",
    "    # Task logic here\n",
    "    logging.info('Task completed')\n",
    "\n",
    "pipeline_task()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
