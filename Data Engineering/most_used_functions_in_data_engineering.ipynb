{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a391c846",
   "metadata": {},
   "source": [
    "# Most Used Functions in Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0535aa",
   "metadata": {},
   "source": [
    "Data engineering involves various tasks such as data ingestion, transformation, validation, and storage. Here, we'll look at some of the most commonly used functions and techniques in data engineering using Python and its libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56aa7c",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d4f70",
   "metadata": {},
   "source": [
    "Data ingestion is the process of obtaining and importing data for immediate use. Common sources include databases, CSV files, APIs, and streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading data from a CSV file using pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reading data from a CSV file\n",
    "data = pd.read_csv('example.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a67592",
   "metadata": {},
   "source": [
    "## 2. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e14cff",
   "metadata": {},
   "source": [
    "Data transformation involves converting data from one format or structure into another. This is essential for data cleaning, normalization, and enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f713f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data transformation using pandas\n",
    "# Adding a new column\n",
    "data['new_column'] = data['existing_column'] * 2\n",
    "\n",
    "# Renaming columns\n",
    "data.rename(columns={'old_name': 'new_name'}, inplace=True)\n",
    "\n",
    "# Filtering data\n",
    "filtered_data = data[data['column'] > threshold]\n",
    "print(filtered_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4196344",
   "metadata": {},
   "source": [
    "## 3. Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa161dbd",
   "metadata": {},
   "source": [
    "Data validation ensures the accuracy and quality of data. This can include checking for missing values, ensuring data types are correct, and validating data against business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data validation using pandas\n",
    "# Checking for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Ensuring correct data types\n",
    "data['column'] = data['column'].astype(int)\n",
    "\n",
    "# Validating data against business rules\n",
    "valid_data = data[data['column'] > 0]\n",
    "print(valid_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a87a84",
   "metadata": {},
   "source": [
    "## 4. Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618a327",
   "metadata": {},
   "source": [
    "Data storage involves saving data in a structured format for future use. Common storage options include relational databases, NoSQL databases, and data lakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Saving data to a database using SQLAlchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Creating an engine and saving data to a SQL database\n",
    "engine = create_engine('sqlite:///example.db')\n",
    "data.to_sql('table_name', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a0515",
   "metadata": {},
   "source": [
    "## 5. Data Pipeline Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e73fb0",
   "metadata": {},
   "source": [
    "Data pipeline orchestration involves scheduling and managing data workflows. Tools like Apache Airflow and Prefect are commonly used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple data pipeline using Airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def my_task():\n",
    "    print('Task executed!')\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'my_dag',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    ")\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='my_task',\n",
    "    python_callable=my_task,\n",
    "    dag=dag,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
