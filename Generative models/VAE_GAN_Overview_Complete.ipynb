{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc77930b",
   "metadata": {},
   "source": [
    "\n",
    "# Variational Autoencoder-GAN (VAE-GAN): A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Variational Autoencoder-GAN (VAE-GAN) architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e93d3e",
   "metadata": {},
   "source": [
    "\n",
    "## History of Variational Autoencoder-GAN (VAE-GAN)\n",
    "\n",
    "Variational Autoencoder-GAN (VAE-GAN) is a hybrid model that combines the strengths of Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). VAE-GAN was introduced to leverage the advantages of both models: the probabilistic latent space representation of VAEs and the sharp image generation capabilities of GANs. The integration of these two models allows for better generative performance, particularly in generating high-quality, realistic images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5912a71",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of VAE-GAN\n",
    "\n",
    "### Architecture\n",
    "\n",
    "VAE-GAN consists of three main components:\n",
    "\n",
    "1. **Encoder (VAE part)**: The Encoder \\( q(z|x) \\) encodes the input data \\( x \\) into a latent variable \\( z \\) using the standard VAE encoding process.\n",
    "\n",
    "\\[\n",
    "z \\sim q(z|x) = \\mathcal{N}(\\mu(x), \\sigma^2(x))\n",
    "\\]\n",
    "\n",
    "2. **Decoder/Generator (VAE-GAN part)**: The Decoder \\( G(z) \\), also acting as the Generator, decodes the latent variable \\( z \\) back into the data space to generate synthetic data \\( G(z) \\).\n",
    "\n",
    "\\[\n",
    "\\hat{x} = G(z)\n",
    "\\]\n",
    "\n",
    "3. **Discriminator (GAN part)**: The Discriminator \\( D(x) \\) distinguishes between real data \\( x \\) and generated data \\( \\hat{x} \\).\n",
    "\n",
    "\\[\n",
    "D(x) = P(\\text{real} | x)\n",
    "\\]\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "VAE-GAN combines the losses of VAE and GAN:\n",
    "\n",
    "1. **VAE Loss**:\n",
    "   - **Reconstruction Loss**: Measures how well the generated data \\( \\hat{x} \\) matches the input data \\( x \\).\n",
    "\n",
    "   \\[\n",
    "   \\mathcal{L}_{\\text{recon}} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)]\n",
    "   \\]\n",
    "\n",
    "   - **KL Divergence**: Regularizes the latent space by ensuring that \\( q(z|x) \\) is close to the prior \\( p(z) \\).\n",
    "\n",
    "   \\[\n",
    "   \\mathcal{L}_{\\text{KL}} = D_{KL}(q(z|x) \\| p(z)) = \\frac{1}{2} \\sum_{i=1}^{n} (1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2)\n",
    "   \\]\n",
    "\n",
    "2. **GAN Loss**:\n",
    "   - **Adversarial Loss**: Ensures that the generated data \\( \\hat{x} \\) is indistinguishable from real data \\( x \\).\n",
    "\n",
    "   \\[\n",
    "   \\mathcal{L}_{\\text{GAN}} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim q(z|x)}[\\log(1 - D(G(z)))]\n",
    "   \\]\n",
    "\n",
    "The overall loss function is a combination of these:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{VAE-GAN}} = \\mathcal{L}_{\\text{recon}} + \\mathcal{L}_{\\text{KL}} + \\mathcal{L}_{\\text{GAN}}\n",
    "\\]\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a VAE-GAN involves alternately optimizing the Encoder, Decoder/Generator, and Discriminator using gradient descent. The Encoder and Decoder are trained to minimize the VAE and GAN losses, while the Discriminator is trained to distinguish real data from generated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052fb6e",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a Variational Autoencoder-GAN (VAE-GAN) using TensorFlow and Keras on the MNIST dataset. The implementation will demonstrate how to combine the VAE and GAN models to create a hybrid architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19afe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "# Encoder (VAE part)\n",
    "inputs = layers.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# Sampling layer\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# Decoder/Generator (VAE-GAN part)\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation='relu')(decoder_input)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
    "outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "encoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = models.Model(decoder_input, outputs, name='decoder')\n",
    "generated_img = decoder(encoder(inputs)[2])\n",
    "\n",
    "# Discriminator (GAN part)\n",
    "discriminator = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=(28, 28, 1)),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'),\n",
    "    layers.LeakyReLU(alpha=0.2),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Combine VAE and GAN\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(generated_img)\n",
    "\n",
    "vae_gan = models.Model(inputs, validity)\n",
    "vae_gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Loss Function for VAE-GAN\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, generated_img)\n",
    "reconstruction_loss *= 28 * 28\n",
    "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "vae_gan.add_loss(vae_loss)\n",
    "\n",
    "# Train the model\n",
    "def train(epochs, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        imgs = x_train[idx]\n",
    "\n",
    "        z = np.random.normal(size=(batch_size, latent_dim))\n",
    "        generated_imgs = decoder.predict(z)\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        vae_gan_loss = vae_gan.train_on_batch(imgs, valid)\n",
    "\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [VAE-GAN loss: {vae_gan_loss}]\")\n",
    "\n",
    "train(epochs=10000, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331dbf3",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Variational Autoencoder-GAN\n",
    "\n",
    "### Advantages\n",
    "- **High-Quality Image Generation**: VAE-GANs combine the strengths of VAEs and GANs to generate high-quality, realistic images.\n",
    "- **Rich Latent Space**: The VAE component ensures a structured and interpretable latent space, allowing for better control over the generated data.\n",
    "\n",
    "### Disadvantages\n",
    "- **Complexity**: VAE-GANs are more complex to implement and train compared to standalone VAEs or GANs, requiring careful balancing of the losses.\n",
    "- **Training Instability**: Like GANs, VAE-GANs can suffer from training instability and mode collapse, making the training process more challenging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce014a",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Variational Autoencoder-GAN (VAE-GAN) is a powerful hybrid model that leverages the strengths of both VAEs and GANs to generate high-quality, realistic images. While the model adds complexity and can be challenging to train, the combination of a structured latent space and sharp image generation capabilities makes VAE-GANs highly valuable for various generative modeling tasks.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
