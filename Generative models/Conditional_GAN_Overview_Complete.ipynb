{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ac6561",
   "metadata": {},
   "source": [
    "\n",
    "# Conditional Generative Adversarial Network (cGAN): A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Conditional Generative Adversarial Network (cGAN) architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baabf1e",
   "metadata": {},
   "source": [
    "\n",
    "## History of Conditional Generative Adversarial Networks (cGANs)\n",
    "\n",
    "Conditional Generative Adversarial Networks (cGANs) were introduced by Mehdi Mirza and Simon Osindero in their 2014 paper \"Conditional Generative Adversarial Nets.\" cGANs are an extension of the traditional GAN model, where both the Generator and Discriminator are conditioned on additional information. This information can be labels, class information, or any other auxiliary data that influences the generation process. The conditional approach allows for more control over the generated data, enabling the creation of specific outputs, such as generating images of specific classes or attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7756e",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Conditional GANs\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A Conditional GAN extends the traditional GAN architecture by conditioning both the Generator and Discriminator on some auxiliary information \\( y \\):\n",
    "\n",
    "1. **Generator**: The Generator \\( G(z|y) \\) takes a noise vector \\( z \\) and the conditional information \\( y \\) as input and generates synthetic data \\( G(z|y) \\).\n",
    "\n",
    "\\[\n",
    "G(z|y) = \\text{synthetic data conditioned on } y\n",
    "\\]\n",
    "\n",
    "2. **Discriminator**: The Discriminator \\( D(x|y) \\) takes both the data \\( x \\) (real or synthetic) and the conditional information \\( y \\) as input and outputs the probability that the data is real.\n",
    "\n",
    "\\[\n",
    "D(x|y) = P(\\text{real} | x, y)\n",
    "\\]\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The objective of cGANs is to solve the following minimax problem:\n",
    "\n",
    "\\[\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_\\text{data}(x)}[\\log D(x|y)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z|y)|y))]\n",
    "\\]\n",
    "\n",
    "- The Discriminator tries to maximize the probability of correctly distinguishing real data from fake data, given the condition \\( y \\).\n",
    "- The Generator tries to minimize the probability that the Discriminator correctly classifies the fake data as fake, conditioned on \\( y \\).\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a cGAN involves alternating between optimizing the Discriminator and Generator using gradient descent, with both networks conditioned on the auxiliary information \\( y \\). The conditional information provides the model with additional context, enabling it to generate data with specific attributes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85f317",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a Conditional Generative Adversarial Network (cGAN) using TensorFlow and Keras on the MNIST dataset, where the Generator will be conditioned on the digit labels to generate specific digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27139ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "x_train = (x_train.astype('float32') - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    noise = layers.Input(shape=(100,))\n",
    "    label = layers.Input(shape=(10,))\n",
    "    model_input = layers.Concatenate()([noise, label])\n",
    "\n",
    "    x = layers.Dense(256)(model_input)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x = layers.Dense(28 * 28 * 1, activation='tanh')(x)\n",
    "    img = layers.Reshape((28, 28, 1))(x)\n",
    "\n",
    "    model = models.Model([noise, label], img)\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    img = layers.Input(shape=(28, 28, 1))\n",
    "    label = layers.Input(shape=(10,))\n",
    "    label_layer = layers.Dense(28 * 28 * 1)(label)\n",
    "    label_layer = layers.Reshape((28, 28, 1))(label_layer)\n",
    "\n",
    "    model_input = layers.Concatenate()([img, label_layer])\n",
    "\n",
    "    x = layers.Flatten()(model_input)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    validity = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model([img, label], validity)\n",
    "    return model\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Create the GAN model\n",
    "noise = layers.Input(shape=(100,))\n",
    "label = layers.Input(shape=(10,))\n",
    "img = generator([noise, label])\n",
    "discriminator.trainable = False\n",
    "valid = discriminator([img, label])\n",
    "\n",
    "cgan = models.Model([noise, label], valid)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training the cGAN\n",
    "def train_cgan(epochs, batch_size=128, save_interval=200):\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
    "        imgs, labels = x_train[idx], y_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        gen_labels = np.random.randint(0, 10, half_batch)\n",
    "        gen_labels = tf.keras.utils.to_categorical(gen_labels, 10)\n",
    "        gen_imgs = generator.predict([noise, gen_labels])\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch([imgs, labels], np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch([gen_imgs, gen_labels], np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        valid_y = np.ones((batch_size, 1))\n",
    "        sampled_labels = np.random.randint(0, 10, batch_size)\n",
    "        sampled_labels = tf.keras.utils.to_categorical(sampled_labels, 10)\n",
    "\n",
    "        g_loss = cgan.train_on_batch([noise, sampled_labels], valid_y)\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            save_imgs(epoch)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 2, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "    sampled_labels = tf.keras.utils.to_categorical(sampled_labels, 10)\n",
    "    gen_imgs = generator.predict([noise, sampled_labels])\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].set_title(f\"Digit: {cnt}\")\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "\n",
    "train_cgan(epochs=10000, batch_size=64, save_interval=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63d6ab",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Conditional GANs\n",
    "\n",
    "### Advantages\n",
    "- **Controlled Data Generation**: cGANs allow for controlled generation of data by conditioning the output on auxiliary information, making them suitable for tasks like image-to-image translation and text-to-image synthesis.\n",
    "- **Versatility**: The conditional framework can be applied to a wide range of applications, providing more flexibility than traditional GANs.\n",
    "\n",
    "### Disadvantages\n",
    "- **Complexity**: cGANs add complexity to the training process due to the additional conditioning information, which may require more careful tuning.\n",
    "- **Training Instability**: Like traditional GANs, cGANs can suffer from training instability and mode collapse, which may be exacerbated by the conditional information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872a323",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Conditional Generative Adversarial Networks (cGANs) provide a powerful extension to the GAN framework by allowing for controlled data generation based on auxiliary information. This makes them highly valuable in tasks where specific attributes of the generated data need to be controlled. While cGANs inherit some of the challenges of traditional GANs, their versatility and ability to generate conditioned data make them a crucial tool in modern machine learning.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
