{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a4db01",
   "metadata": {},
   "source": [
    "# Most Used Functions in Text-to-Image Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c2a45",
   "metadata": {},
   "source": [
    "Text-to-image models generate images from textual descriptions. These models typically use a combination of Natural Language Processing (NLP) and Computer Vision techniques. In this notebook, we will cover some of the most commonly used functions and techniques for implementing a simplified version of a text-to-image model using TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587f273",
   "metadata": {},
   "source": [
    "## 1. Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ab6c0",
   "metadata": {},
   "source": [
    "Text-to-image models require paired text and image data. For simplicity, we'll use a small synthetic dataset. In practice, you would use a larger dataset like MS COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample data: Text descriptions and corresponding images (randomly generated for this example)\n",
    "texts = [\"red square\", \"green circle\", \"blue triangle\"]\n",
    "images = np.random.rand(3, 64, 64, 3)  # Replace with actual image data\n",
    "\n",
    "# Text preprocessing\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, padding='post')\n",
    "\n",
    "print(\"Text sequences:\", text_sequences)\n",
    "print(\"Images shape:\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508df843",
   "metadata": {},
   "source": [
    "## 2. Building the Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683ee21",
   "metadata": {},
   "source": [
    "The text encoder processes the input text and generates a latent representation. Here, we use an Embedding layer followed by an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d568a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "\n",
    "# Function to build the text encoder\n",
    "def build_text_encoder(vocab_size, embedding_dim, lstm_units):\n",
    "    inputs = Input(shape=(None,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "    x = LSTM(lstm_units)(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the text encoder\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "lstm_units = 100\n",
    "text_encoder = build_text_encoder(vocab_size, embedding_dim, lstm_units)\n",
    "text_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fb250",
   "metadata": {},
   "source": [
    "## 3. Building the Image Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57812b7c",
   "metadata": {},
   "source": [
    "The image decoder generates images from the latent representation of the text. It uses a series of convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44812164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2DTranspose, Reshape, Activation\n",
    "\n",
    "# Function to build the image decoder\n",
    "def build_image_decoder(latent_dim):\n",
    "    inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(8 * 8 * 128, activation='relu')(inputs)\n",
    "    x = Reshape((8, 8, 128))(x)\n",
    "    x = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the image decoder\n",
    "latent_dim = lstm_units\n",
    "image_decoder = build_image_decoder(latent_dim)\n",
    "image_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49cad6",
   "metadata": {},
   "source": [
    "## 4. Building and Compiling the Text-to-Image Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008c785",
   "metadata": {},
   "source": [
    "The text-to-image model combines the text encoder and the image decoder. The model is trained to minimize the difference between the generated and actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the text-to-image model\n",
    "def build_text_to_image_model(text_encoder, image_decoder):\n",
    "    inputs = Input(shape=(None,))\n",
    "    x = text_encoder(inputs)\n",
    "    outputs = image_decoder(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the text-to-image model\n",
    "text_to_image_model = build_text_to_image_model(text_encoder, image_decoder)\n",
    "text_to_image_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f88bf",
   "metadata": {},
   "source": [
    "## 5. Training the Text-to-Image Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4a916",
   "metadata": {},
   "source": [
    "The text-to-image model is trained to generate images that correspond to the input text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the text-to-image model\n",
    "text_to_image_model.fit(text_sequences, images, epochs=10, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065c636",
   "metadata": {},
   "source": [
    "## 6. Generating Images from Text Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d2372",
   "metadata": {},
   "source": [
    "After training, the text-to-image model can generate new images from text descriptions. Here we demonstrate how to generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a227d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an image from a text description\n",
    "def generate_image_from_text(model, text, tokenizer):\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\n",
    "    text_sequence = tf.keras.preprocessing.sequence.pad_sequences(text_sequence, padding='post')\n",
    "    generated_image = model.predict(text_sequence)\n",
    "    return generated_image\n",
    "\n",
    "# Generate an image from a text description\n",
    "text_description = \"red square\"\n",
    "generated_image = generate_image_from_text(text_to_image_model, text_description, tokenizer)\n",
    "print(\"Generated image shape:\", generated_image.shape)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
