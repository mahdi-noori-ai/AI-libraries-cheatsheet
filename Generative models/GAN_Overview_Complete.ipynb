{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eeeb229",
   "metadata": {},
   "source": [
    "\n",
    "# Generative Adversarial Network (GAN): A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Generative Adversarial Network (GAN) architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349dfb7",
   "metadata": {},
   "source": [
    "\n",
    "## History of Generative Adversarial Networks (GANs)\n",
    "\n",
    "Generative Adversarial Networks (GANs) were introduced by Ian Goodfellow and his colleagues in 2014 in the paper \"Generative Adversarial Nets.\" GANs are a class of machine learning frameworks designed for generative modeling. The core idea is to have two neural networks, a Generator and a Discriminator, compete against each other in a zero-sum game. This competition drives the Generator to produce data indistinguishable from real data, leading to the creation of highly realistic synthetic data. GANs have...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d5d49",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Generative Adversarial Networks\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A Generative Adversarial Network consists of two neural networks:\n",
    "\n",
    "1. **Generator**: The Generator \\( G \\) takes a random noise vector \\( z \\) as input and generates synthetic data \\( G(z) \\). The goal of the Generator is to produce data that is indistinguishable from the real data.\n",
    "\n",
    "\\[\n",
    "G(z) = \\text{synthetic data}\n",
    "\\]\n",
    "\n",
    "2. **Discriminator**: The Discriminator \\( D \\) takes both real data \\( x \\) and synthetic data \\( G(z) \\) as input and outputs the probability that the input data is real. The goal of the Discriminator is to correctly classify the real and synthetic data.\n",
    "\n",
    "\\[\n",
    "D(x) = P(\\text{real} | x)\n",
    "\\]\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The objective of GANs is to find a Nash equilibrium between the Generator and Discriminator through the following minimax optimization:\n",
    "\n",
    "\\[\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_\\text{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
    "\\]\n",
    "\n",
    "- The Discriminator tries to maximize the probability of correctly distinguishing real data from fake data.\n",
    "- The Generator tries to minimize the probability that the Discriminator correctly classifies the fake data as fake.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a GAN involves alternating between optimizing the Discriminator and Generator using gradient descent. The Discriminator is trained to distinguish between real and fake data, and the Generator is trained to fool the Discriminator by generating realistic data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e475f8",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simple Generative Adversarial Network (GAN) using TensorFlow and Keras on the MNIST dataset, which consists of handwritten digit images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = (x_train.astype('float32') - 127.5) / 127.5\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=100))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Dense(1024))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(layers.Dense(28 * 28 * 1, activation='tanh'))\n",
    "    model.add(layers.Reshape((28, 28, 1)))\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Create the GAN model\n",
    "z = layers.Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(img)\n",
    "\n",
    "gan = models.Model(z, valid)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training the GAN\n",
    "def train_gan(epochs, batch_size=128, save_interval=200):\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
    "        imgs = x_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        valid_y = np.array([1] * batch_size)\n",
    "\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            save_imgs(epoch)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "\n",
    "train_gan(epochs=10000, batch_size=64, save_interval=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12e383",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Generative Adversarial Networks\n",
    "\n",
    "### Advantages\n",
    "- **High-Quality Data Generation**: GANs are capable of generating highly realistic data, such as images, videos, and audio.\n",
    "- **Wide Range of Applications**: GANs are used in various fields, including image synthesis, data augmentation, super-resolution, and even drug discovery.\n",
    "\n",
    "### Disadvantages\n",
    "- **Training Instability**: Training GANs can be challenging due to issues like mode collapse, where the Generator produces limited varieties of output.\n",
    "- **Sensitive to Hyperparameters**: GANs are sensitive to the choice of hyperparameters and often require extensive tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ddda3",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Generative Adversarial Networks (GANs) have revolutionized the field of generative modeling, enabling the creation of highly realistic synthetic data. Despite their complexity and the challenges involved in training, GANs are widely used in many cutting-edge applications. Understanding GANs is crucial for anyone interested in the future of machine learning and artificial intelligence.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
