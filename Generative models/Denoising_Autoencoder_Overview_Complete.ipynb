{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b27256",
   "metadata": {},
   "source": [
    "\n",
    "# Denoising Autoencoder: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of the Denoising Autoencoder architecture, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1176cd",
   "metadata": {},
   "source": [
    "\n",
    "## History of Denoising Autoencoders\n",
    "\n",
    "The Denoising Autoencoder (DAE) was introduced by Pascal Vincent et al. in 2008 as an extension of the basic autoencoder. The primary idea behind DAEs is to learn a robust representation by training the model to reconstruct the original input from a corrupted version of it. This method improves the model's ability to generalize and is particularly useful for tasks such as image denoising and feature learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388f9ba",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Denoising Autoencoders\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A Denoising Autoencoder is similar to a Vanilla Autoencoder but with an additional step of adding noise to the input data. The architecture consists of two main components:\n",
    "\n",
    "1. **Encoder**: The encoder compresses the noisy input data \\( \\tilde{x} \\) into a lower-dimensional representation \\( z \\).\n",
    "\n",
    "\\[\n",
    "z = f(\\tilde{x}) = \\sigma(W\\tilde{x} + b)\n",
    "\\]\n",
    "\n",
    "Where \\( \\tilde{x} \\) is the noisy input.\n",
    "\n",
    "2. **Decoder**: The decoder reconstructs the original input data \\( x \\) from the latent representation \\( z \\).\n",
    "\n",
    "\\[\n",
    "\\hat{x} = g(z) = \\sigma'(W'z + b')\n",
    "\\]\n",
    "\n",
    "### Noise Addition\n",
    "\n",
    "The noise is typically added to the input data by randomly corrupting some of the input values. Common methods include Gaussian noise, masking noise (setting random inputs to zero), or salt-and-pepper noise.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The objective of a Denoising Autoencoder is to minimize the reconstruction error between the original input \\( x \\) and the reconstructed output \\( \\hat{x} \\):\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
    "\\]\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a Denoising Autoencoder involves backpropagation to minimize the reconstruction loss, updating the weights of both the encoder and decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfcc2d",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a Denoising Autoencoder using TensorFlow and Keras on the MNIST dataset, adding Gaussian noise to the input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6711fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), 28, 28, 1))\n",
    "x_test = x_test.reshape((len(x_test), 28, 28, 1))\n",
    "\n",
    "# Add Gaussian noise to the images\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Define the Denoising Autoencoder model\n",
    "input_img = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "encoded = layers.Flatten()(input_img)\n",
    "encoded = layers.Dense(128, activation='relu')(encoded)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(28 * 28, activation='sigmoid')(decoded)\n",
    "decoded = layers.Reshape((28, 28, 1))(decoded)\n",
    "\n",
    "# Build the model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(x_train_noisy, x_train, epochs=10, batch_size=256, validation_data=(x_test_noisy, x_test))\n",
    "\n",
    "# Evaluate the model\n",
    "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "# Plot original, noisy, and reconstructed images\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Noisy\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Reconstructed\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89344ca",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Denoising Autoencoder\n",
    "\n",
    "### Advantages\n",
    "- **Robust Feature Learning**: Denoising Autoencoders learn robust features by reconstructing the original input from corrupted data, making them more resilient to noise.\n",
    "- **Improved Generalization**: The addition of noise during training helps in regularizing the model, reducing overfitting and improving generalization.\n",
    "\n",
    "### Disadvantages\n",
    "- **Increased Complexity**: The process of adding noise and training the model to denoise adds complexity to the implementation.\n",
    "- **Sensitive to Noise Type**: The performance of the Denoising Autoencoder can be sensitive to the type and amount of noise added to the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55643006",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Denoising Autoencoders are a powerful extension of the basic autoencoder, capable of learning robust representations even in the presence of noise. They are widely used in applications like image denoising and feature extraction, making them an important tool in the deep learning arsenal. While they add complexity to the training process, their ability to improve generalization makes them highly valuable for unsupervised learning tasks.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
