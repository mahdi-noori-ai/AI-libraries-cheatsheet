{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b597a571",
   "metadata": {},
   "source": [
    "\n",
    "# Capsule Networks: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Capsule Networks, including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c486524",
   "metadata": {},
   "source": [
    "\n",
    "## History of Capsule Networks\n",
    "\n",
    "Capsule Networks were introduced by Geoffrey Hinton and his colleagues in 2017 in the paper \"Dynamic Routing Between Capsules.\" The idea behind Capsule Networks was to address the limitations of Convolutional Neural Networks (CNNs) in recognizing spatial hierarchies in visual data. Capsule Networks aim to preserve the spatial relationships between features through the use of capsules, which are groups of neurons that output a vector rather than a scalar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cf97f",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Capsule Networks\n",
    "\n",
    "### Capsule Structure\n",
    "\n",
    "A Capsule is a group of neurons whose output is a vector rather than a scalar. The length of the vector represents the probability that an entity is present, and the orientation of the vector encodes the instantiation parameters (e.g., pose, size, and orientation).\n",
    "\n",
    "1. **Squashing Function**: The squashing function ensures that short vectors get shrunk to almost zero length and long vectors get shrunk to a length slightly below 1.\n",
    "\n",
    "\\[\n",
    "v_j = \\frac{\\|s_j\\|^2}{1 + \\|s_j\\|^2} \\frac{s_j}{\\|s_j\\|}\n",
    "\\]\n",
    "\n",
    "Where \\( s_j \\) is the total input to the capsule, and \\( v_j \\) is the output of the capsule.\n",
    "\n",
    "2. **Dynamic Routing**: Capsules use a mechanism called dynamic routing to ensure that the output of lower-level capsules is sent to the appropriate higher-level capsules. This routing mechanism replaces the max-pooling operation in CNNs, which often loses important spatial information.\n",
    "\n",
    "\\[\n",
    "c_{ij} = \\text{softmax}(b_{ij})\n",
    "\\]\n",
    "\n",
    "Where \\( c_{ij} \\) are the coupling coefficients that determine how much influence a lower-level capsule \\( i \\) has on a higher-level capsule \\( j \\).\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Capsule Networks use a margin loss for training, which ensures that the length of the output vector of the correct class capsule is close to 1, and the lengths of the output vectors of all other class capsules are close to 0.\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = T_c \\max(0, m^+ - \\|v_c\\|)^2 + \\lambda(1 - T_c) \\max(0, \\|v_c\\| - m^-)^2\n",
    "\\]\n",
    "\n",
    "Where \\( T_c \\) is 1 if the entity is present and 0 otherwise, \\( m^+ \\) and \\( m^- \\) are the margins, and \\( \\lambda \\) is a weighting parameter.\n",
    "\n",
    "### Reconstruction Regularizer\n",
    "\n",
    "To encourage the network to encode detailed information, a reconstruction network is often attached to the output layer. The reconstruction loss is typically the mean squared error between the input image and the reconstructed image from the capsule's output.\n",
    "\n",
    "\\[\n",
    "\\text{Reconstruction Loss} = \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
    "\\]\n",
    "\n",
    "### Training\n",
    "\n",
    "Training a Capsule Network involves backpropagation to minimize the combined margin loss and reconstruction loss, updating the weights of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c968c",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simple Capsule Network using TensorFlow and Keras on the MNIST dataset, which consists of handwritten digit images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5eac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Define the Capsule Network model\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsules, num_routing=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsules = dim_capsules\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=[self.num_capsules, input_shape[1], self.dim_capsules, input_shape[2]],\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_expand = tf.expand_dims(inputs, 1)\n",
    "        inputs_tile = tf.expand_dims(inputs_expand, 2)\n",
    "        inputs_tiled = tf.tile(inputs_tile, [1, self.num_capsules, 1, 1, 1])\n",
    "        inputs_hat = tf.keras.backend.map_fn(lambda x: tf.keras.backend.batch_dot(x, self.W, [3, 2]), elems=inputs_tiled)\n",
    "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.num_capsules, inputs.shape[1]])\n",
    "\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(b, axis=1)\n",
    "            outputs = squash(tf.keras.backend.batch_dot(c, inputs_hat, [2, 2]))\n",
    "            if i < self.num_routing - 1:\n",
    "                b += tf.keras.backend.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "        return outputs\n",
    "\n",
    "input_layer = layers.Input(shape=(28, 28, 1))\n",
    "conv1 = layers.Conv2D(256, (9, 9), strides=(1, 1), activation='relu')(input_layer)\n",
    "conv2 = layers.Conv2D(256, (9, 9), strides=(2, 2), activation='relu')(conv1)\n",
    "conv2_reshaped = layers.Reshape((-1, 256))(conv2)\n",
    "capsule_layer = CapsuleLayer(num_capsules=10, dim_capsules=16, num_routing=3)(conv2_reshaped)\n",
    "output_capsule = layers.Lambda(lambda z: tf.sqrt(tf.reduce_sum(tf.square(z), axis=2)))(capsule_layer)\n",
    "\n",
    "# Define the model\n",
    "model = models.Model(inputs=input_layer, outputs=output_capsule)\n",
    "model.compile(optimizer='adam', loss='margin_loss', metrics=['accuracy'])\n",
    "\n",
    "# Define the margin loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * tf.square(tf.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * tf.square(tf.maximum(0., y_pred - 0.1))\n",
    "    return tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "# Train the model\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=128, epochs=10, validation_data=(x_test, y_test_one_hot))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Plot some test images and their predictions\n",
    "n_images = 10\n",
    "test_images = x_test[:n_images]\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n_images):\n",
    "    plt.subplot(2, n_images, i + 1)\n",
    "    plt.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2, n_images, i + 1 + n_images)\n",
    "    plt.bar(range(10), predictions[i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7b8bd",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Capsule Networks\n",
    "\n",
    "### Advantages\n",
    "- **Preservation of Spatial Hierarchy**: Capsule Networks preserve the spatial relationships between features, which can lead to better generalization, especially in tasks that require an understanding of spatial hierarchies.\n",
    "- **Dynamic Routing**: The dynamic routing mechanism allows for more flexible and interpretable connections between layers, reducing the need for max-pooling.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Complexity**: Capsule Networks are more computationally intensive than traditional CNNs, both in terms of training time and memory requirements.\n",
    "- **Training Challenges**: The dynamic routing algorithm can be difficult to train and may require careful tuning of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370bfdd",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Capsule Networks represent a significant advancement over traditional CNNs by preserving spatial hierarchies and introducing dynamic routing between layers. While they offer several advantages, such as improved generalization and interpretability, they also come with challenges related to computational complexity and training stability. Capsule Networks continue to be an active area of research, with ongoing efforts to improve their efficiency and performance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
