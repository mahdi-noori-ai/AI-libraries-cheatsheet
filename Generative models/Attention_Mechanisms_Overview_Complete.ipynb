{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83b6c66",
   "metadata": {},
   "source": [
    "\n",
    "# Attention Mechanisms: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Attention Mechanisms, including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251070e",
   "metadata": {},
   "source": [
    "\n",
    "## History of Attention Mechanisms\n",
    "\n",
    "Attention mechanisms were introduced by Bahdanau et al. in 2014 in the context of Neural Machine Translation (NMT) in the paper \"Neural Machine Translation by Jointly Learning to Align and Translate.\" The attention mechanism was designed to address the limitations of the encoder-decoder architecture by allowing the model to focus on specific parts of the input sequence when making predictions. Since then, attention mechanisms have become a fundamental component in various deep learning models, most notably in the Transformer architecture, which has revolutionized the field of Natural Language Processing (NLP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504ad50",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Attention Mechanisms\n",
    "\n",
    "### Basic Attention Mechanism\n",
    "\n",
    "The attention mechanism computes a weighted sum of the input sequence, where the weights are determined by a learned alignment between the input and output sequences.\n",
    "\n",
    "1. **Score Function**: The score function calculates the alignment score between the current decoder state \\( s_t \\) and each encoder hidden state \\( h_i \\).\n",
    "\n",
    "\\[\n",
    "\\text{score}(s_t, h_i) = s_t^\\top W_a h_i\n",
    "\\]\n",
    "\n",
    "Where \\( W_a \\) is a weight matrix.\n",
    "\n",
    "2. **Attention Weights**: The attention weights \\( \\alpha_i \\) are computed by applying a softmax function to the alignment scores.\n",
    "\n",
    "\\[\n",
    "\\alpha_i = \\frac{\\exp(\\text{score}(s_t, h_i))}{\\sum_{j} \\exp(\\text{score}(s_t, h_j))}\n",
    "\\]\n",
    "\n",
    "3. **Context Vector**: The context vector \\( c_t \\) is computed as the weighted sum of the encoder hidden states.\n",
    "\n",
    "\\[\n",
    "c_t = \\sum_{i} \\alpha_i h_i\n",
    "\\]\n",
    "\n",
    "4. **Attention Output**: The context vector \\( c_t \\) is then combined with the decoder state to produce the final output.\n",
    "\n",
    "\\[\n",
    "\\tilde{s}_t = \\tanh(W_c [c_t; s_t])\n",
    "\\]\n",
    "\n",
    "Where \\( W_c \\) is a weight matrix, and \\( [c_t; s_t] \\) denotes the concatenation of \\( c_t \\) and \\( s_t \\).\n",
    "\n",
    "### Self-Attention Mechanism\n",
    "\n",
    "Self-attention is a type of attention mechanism where the input sequence is compared with itself to compute the attention weights.\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The self-attention mechanism computes the attention weights using the scaled dot-product of the query \\( Q \\), key \\( K \\), and value \\( V \\) matrices.\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "\\]\n",
    "\n",
    "Where \\( d_k \\) is the dimension of the key vectors.\n",
    "\n",
    "2. **Multi-Head Attention**: In multi-head attention, the input is split into multiple heads, and each head performs self-attention separately. The results are then concatenated and linearly transformed.\n",
    "\n",
    "\\[\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h) W_o\n",
    "\\]\n",
    "\n",
    "Where \\( W_o \\) is a weight matrix.\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "The Transformer architecture, introduced by Vaswani et al. in 2017, relies entirely on self-attention mechanisms to capture dependencies in the input sequence. It consists of an encoder-decoder structure, with both components using self-attention layers and feedforward networks.\n",
    "\n",
    "\\[\n",
    "\\text{Transformer Encoder} = \\text{Self-Attention} + \\text{Feedforward Network}\n",
    "\\]\n",
    "\\[\n",
    "\\text{Transformer Decoder} = \\text{Self-Attention} + \\text{Encoder-Decoder Attention} + \\text{Feedforward Network}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a53f99",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic example of using an attention mechanism in a sequence-to-sequence model using TensorFlow and Keras. The example will focus on using attention for a machine translation task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (this is a simplified example)\n",
    "input_texts = ['hello', 'how are you', 'good morning']\n",
    "target_texts = ['hola', 'cómo estás', 'buenos días']\n",
    "\n",
    "# Vocabulary\n",
    "input_vocab = sorted(set(' '.join(input_texts)))\n",
    "target_vocab = sorted(set(' '.join(target_texts)))\n",
    "\n",
    "# Create a simple tokenization\n",
    "input_tokenizer = {char: idx + 1 for idx, char in enumerate(input_vocab)}\n",
    "target_tokenizer = {char: idx + 1 for idx, char in enumerate(target_vocab)}\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tokenizer[char] for char in text]\n",
    "\n",
    "input_sequences = [tokenize(text, input_tokenizer) for text in input_texts]\n",
    "target_sequences = [tokenize(text, target_tokenizer) for text in target_texts]\n",
    "\n",
    "# Pad sequences\n",
    "max_len_input = max(len(seq) for seq in input_sequences)\n",
    "max_len_target = max(len(seq) for seq in target_sequences)\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_len_input, padding='post')\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
    "\n",
    "# Define the model with attention\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_a = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.U_a = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.V_a = self.add_weight(shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, encoder_output, decoder_output):\n",
    "        score = tf.nn.tanh(tf.tensordot(encoder_output, self.W_a, axes=[2, 0]) +\n",
    "                           tf.tensordot(decoder_output, self.U_a, axes=[2, 0]))\n",
    "        attention_weights = tf.nn.softmax(tf.tensordot(score, self.V_a, axes=[2, 0]), axis=1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=(max_len_input,))\n",
    "encoder_embedding = layers.Embedding(input_dim=len(input_vocab) + 1, output_dim=64)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(64, return_sequences=True, return_state=True)(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.Input(shape=(max_len_target,))\n",
    "decoder_embedding = layers.Embedding(input_dim=len(target_vocab) + 1, output_dim=64)(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention = AttentionLayer()\n",
    "context_vector, attention_weights = attention(encoder_outputs, decoder_outputs)\n",
    "\n",
    "# Concatenate context vector and decoder LSTM output\n",
    "decoder_combined_context = layers.Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "\n",
    "# Output layer\n",
    "output = layers.TimeDistributed(layers.Dense(len(target_vocab) + 1, activation='softmax'))(decoder_combined_context)\n",
    "\n",
    "# Define the model\n",
    "model = models.Model([encoder_inputs, decoder_inputs], output)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy training (this is a toy example, so training is not performed)\n",
    "# model.fit([input_sequences, target_sequences], target_sequences, epochs=10)\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc019d1c",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Attention Mechanisms\n",
    "\n",
    "### Advantages\n",
    "- **Improved Performance**: Attention mechanisms have significantly improved the performance of models on tasks such as machine translation, text summarization, and image captioning.\n",
    "- **Interpretability**: The attention weights provide insights into which parts of the input the model is focusing on, making the model more interpretable.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Overhead**: Attention mechanisms add computational complexity, especially in models with long input sequences.\n",
    "- **Memory Usage**: The self-attention mechanism, in particular, can be memory-intensive, as it requires computing and storing large attention matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a92300e",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Attention mechanisms have revolutionized the field of deep learning, particularly in Natural Language Processing and Computer Vision. Their ability to focus on relevant parts of the input, combined with the flexibility of self-attention, has made them a fundamental component in modern neural network architectures, such as Transformers. While attention mechanisms come with challenges related to computational resources, their benefits in terms of performance and interpretability make them indispensable in man...\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
