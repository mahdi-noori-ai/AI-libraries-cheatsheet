{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55512e9",
   "metadata": {},
   "source": [
    "\n",
    "# BERT (Bidirectional Encoder Representations from Transformers): A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of BERT (Bidirectional Encoder Representations from Transformers), including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1d012",
   "metadata": {},
   "source": [
    "\n",
    "## History of BERT\n",
    "\n",
    "BERT was introduced by Jacob Devlin et al. in 2018 through the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" BERT represented a significant breakthrough in Natural Language Processing (NLP) as it allowed for pre-training a deep bidirectional transformer on large text corpora, which could then be fine-tuned for a variety of NLP tasks. BERT's ability to consider the context of words from both the left and right sides in a sentence marked a major advancement over previous models that processed text either left-to-right or right-to-left.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586daf34",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of BERT\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "BERT is based on the Transformer architecture, specifically the Encoder part of the Transformer.\n",
    "\n",
    "1. **Self-Attention Mechanism**: The self-attention mechanism in Transformers allows the model to weigh the importance of different words in a sentence when constructing word representations.\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "Where \\( Q \\), \\( K \\), and \\( V \\) are the query, key, and value matrices, respectively, and \\( d_k \\) is the dimension of the key vectors.\n",
    "\n",
    "2. **Bidirectional Context**: BERT uses a bidirectional training approach to consider the context from both directions (left-to-right and right-to-left) when encoding words, unlike traditional models.\n",
    "\n",
    "3. **Masked Language Model (MLM)**: BERT is pre-trained using the Masked Language Model objective, where some words in the input are randomly masked, and the model learns to predict the masked words based on the surrounding context.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MLM}} = -\\sum_{t=1}^{T} \\log P(w_t | w_{1:t-1}, w_{t+1:T})\n",
    "\\]\n",
    "\n",
    "4. **Next Sentence Prediction (NSP)**: Another pre-training task for BERT is the Next Sentence Prediction, where the model learns to predict whether a given sentence follows another sentence in the original text.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{NSP}} = -\\log P(\\text{IsNext} | S_1, S_2)\n",
    "\\]\n",
    "\n",
    "### Fine-Tuning\n",
    "\n",
    "After pre-training, BERT can be fine-tuned on specific tasks by adding a task-specific output layer, which is trained using the labeled data for that task. Fine-tuning allows BERT to achieve state-of-the-art results on various NLP tasks such as text classification, question answering, and named entity recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14273ed9",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic example of using BERT for a text classification task using the Hugging Face Transformers library. The dataset we'll use is the IMDb movie reviews dataset, where the task is to classify reviews as positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers datasets\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the dataset for training\n",
    "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in list(range(1000))])\n",
    "small_test_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in list(range(1000))])\n",
    "\n",
    "# Load BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(small_train_dataset['input_ids'], small_train_dataset['label'], epochs=3, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(small_test_dataset['input_ids'], small_test_dataset['label'])\n",
    "\n",
    "# Use the model for inference\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "result = classifier(\"This movie was absolutely fantastic!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6a7e91",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of BERT\n",
    "\n",
    "### Advantages\n",
    "- **State-of-the-Art Performance**: BERT has achieved state-of-the-art results on a wide range of NLP tasks, including text classification, question answering, and named entity recognition.\n",
    "- **Pre-trained Representations**: BERT's pre-trained representations can be fine-tuned on specific tasks with relatively small amounts of labeled data, making it highly versatile.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computationally Intensive**: BERT requires significant computational resources for both pre-training and fine-tuning, making it less accessible for small organizations or researchers with limited resources.\n",
    "- **Large Model Size**: The large model size of BERT can be challenging to deploy in environments with limited computational resources or memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecde9d7",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "BERT has revolutionized the field of Natural Language Processing by introducing a pre-trained bidirectional transformer model that can be fine-tuned for a wide range of NLP tasks. While BERT's performance is impressive, it comes with challenges related to computational resources and model size. Nevertheless, BERT remains a foundational model in NLP and continues to influence the development of new models and techniques in the field.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
