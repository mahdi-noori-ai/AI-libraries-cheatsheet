{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1687265",
   "metadata": {},
   "source": [
    "\n",
    "# Contrastive Networks: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Contrastive Networks, including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa33f4",
   "metadata": {},
   "source": [
    "\n",
    "## History of Contrastive Networks\n",
    "\n",
    "Contrastive Networks, often associated with the broader field of contrastive learning, are designed to learn representations by comparing similar and dissimilar pairs of data points. The concept of contrastive learning can be traced back to models like the Siamese Network, introduced by Bromley and LeCun in 1993. However, the term \"Contrastive Network\" often refers to more recent developments in contrastive learning, such as SimCLR and MoCo, which have shown great promise in unsupervised representation learning and self-supervised learning contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8824c",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Contrastive Networks\n",
    "\n",
    "### Contrastive Learning Objective\n",
    "\n",
    "The core idea behind contrastive learning is to learn an embedding space where similar data points are close together and dissimilar data points are far apart. This is typically achieved using a contrastive loss function.\n",
    "\n",
    "1. **Contrastive Loss**: The contrastive loss encourages the model to learn embeddings that pull similar pairs close and push dissimilar pairs apart.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{contrastive}} = \\sum_{i=1}^{N} \\left( y_i \\cdot \\|f(x_i^1) - f(x_i^2)\\|^2 + (1 - y_i) \\cdot \\max(0, m - \\|f(x_i^1) - f(x_i^2)\\|)^2 \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( x_i^1 \\) and \\( x_i^2 \\) are the two inputs.\n",
    "- \\( y_i \\) is 1 if the inputs are similar and 0 if they are dissimilar.\n",
    "- \\( m \\) is a margin parameter that defines the minimum distance between dissimilar pairs.\n",
    "- \\( f(\\cdot) \\) is the embedding function.\n",
    "\n",
    "### Contrastive Networks in Unsupervised Learning\n",
    "\n",
    "In unsupervised or self-supervised learning, Contrastive Networks use augmentation to generate positive pairs (similar data points) from the same instance and negative pairs (dissimilar data points) from different instances.\n",
    "\n",
    "1. **Data Augmentation**: Augmentations such as random cropping, color jittering, or flipping are applied to generate different views of the same data instance.\n",
    "\n",
    "\\[\n",
    "\\text{Augmentations: } T_1(x), T_2(x) \\text{ for each instance } x\n",
    "\\]\n",
    "\n",
    "2. **SimCLR Objective**: SimCLR is a prominent example of a contrastive learning approach, where the loss function is defined over a large batch with multiple positive and negative pairs.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{SimCLR}} = - \\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( z_i \\) and \\( z_j \\) are embeddings of the augmented instances.\n",
    "- \\( \\text{sim}(\\cdot) \\) is a similarity function (e.g., cosine similarity).\n",
    "- \\( \\tau \\) is a temperature parameter.\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "Training contrastive networks typically requires large batch sizes or memory banks to store embeddings of negative samples. The effectiveness of contrastive learning relies heavily on the choice of augmentations, the number of negative samples, and the similarity metric used in the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872ca6e",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simple Contrastive Network using TensorFlow and Keras for an image similarity task using the MNIST dataset. We'll create pairs of images and use a contrastive loss function to train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Prepare pairs of images and labels for training\n",
    "def create_pairs(x, digit_indices):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "    for d in range(10):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = np.random.randint(1, 10)\n",
    "            dn = (d + inc) % 10\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n",
    "tr_pairs, tr_y = create_pairs(x_train, digit_indices)\n",
    "\n",
    "digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n",
    "te_pairs, te_y = create_pairs(x_test, digit_indices)\n",
    "\n",
    "# Define the Contrastive Network model\n",
    "def create_base_network(input_shape):\n",
    "    input = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(input)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    return models.Model(input, x)\n",
    "\n",
    "base_network = create_base_network((28, 28, 1))\n",
    "\n",
    "input_a = layers.Input(shape=(28, 28, 1))\n",
    "input_b = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = layers.Lambda(lambda embeddings: tf.sqrt(tf.reduce_sum(tf.square(embeddings[0] - embeddings[1]), axis=1, keepdims=True)))([processed_a, processed_b])\n",
    "\n",
    "model = models.Model([input_a, input_b], distance)\n",
    "\n",
    "# Compile the model\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return tf.reduce_mean(y_true * tf.square(y_pred) + (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0)))\n",
    "\n",
    "model.compile(optimizer='adam', loss=contrastive_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y, batch_size=128, epochs=10, validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate([te_pairs[:, 0], te_pairs[:, 1]], te_y)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab7249",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Contrastive Networks\n",
    "\n",
    "### Advantages\n",
    "- **Effective for Unsupervised Learning**: Contrastive Networks, particularly in the form of contrastive learning methods like SimCLR, have shown to be highly effective for learning useful representations without labeled data.\n",
    "- **Versatility**: The learned embeddings can be used for a variety of downstream tasks, making contrastive learning a versatile approach for representation learning.\n",
    "\n",
    "### Disadvantages\n",
    "- **Large Batch Sizes Required**: Effective contrastive learning often requires large batch sizes or memory banks to store negative samples, which can be computationally expensive.\n",
    "- **Sensitive to Augmentation Choices**: The performance of contrastive learning methods can be highly dependent on the choice of augmentations, requiring careful tuning for different tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015926cb",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Contrastive Networks and contrastive learning have become powerful tools for representation learning, particularly in unsupervised settings. By learning to distinguish between similar and dissimilar pairs, these models can create rich embeddings that are useful for a wide range of downstream tasks. While they offer significant advantages, such as effectiveness in unsupervised learning, they also come with challenges related to computational resources and the need for careful tuning of augmentations.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
