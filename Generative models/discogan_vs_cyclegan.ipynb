{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fcc4d3a9",
      "metadata": {
        "id": "fcc4d3a9"
      },
      "source": [
        "# DiscoGAN vs CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d9e7108",
      "metadata": {
        "id": "9d9e7108"
      },
      "source": [
        "DiscoGAN and CycleGAN are both Generative Adversarial Networks (GANs) designed for unsupervised image-to-image translation, but they have some differences in their design and applications. Here’s a detailed comparison of the two."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57cc1f43",
      "metadata": {
        "id": "57cc1f43"
      },
      "source": [
        "## Commonalities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de9cdc4f",
      "metadata": {
        "id": "de9cdc4f"
      },
      "source": [
        "- **Unsupervised Learning**: Both models are used for unsupervised learning, where paired training data is not available.\n",
        "- **Cycle Consistency Loss**: Both models use cycle consistency loss to ensure that translating an image to the target domain and back results in the original image.\n",
        "- **Architecture**: Both employ a pair of generators and a pair of discriminators for their operations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c13fb2e",
      "metadata": {
        "id": "7c13fb2e"
      },
      "source": [
        "## Differences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac606b2",
      "metadata": {
        "id": "fac606b2"
      },
      "source": [
        "| Aspect | DiscoGAN | CycleGAN |\n",
        "|--------|----------|----------|\n",
        "| **Objective** | Focuses on discovering cross-domain relationships and transferring styles between two different domains. | Primarily designed for image-to-image translation tasks where paired examples are not available. |\n",
        "| **Applications** | Commonly used for style transfer, such as transforming objects or styles between different domains (e.g., faces of different genders, artistic styles). | Broadly used for a variety of tasks including artistic style transfer, photo enhancement, object transfiguration, and more. |\n",
        "| **Cycle Consistency Implementation** | Uses cycle consistency loss to ensure that the transformation to the target domain and back results in the original image. | Similar use of cycle consistency loss but may differ in implementation specifics, such as the exact loss functions and architectures used. |\n",
        "| **Loss Functions** | Uses a combination of adversarial loss and cycle consistency loss. The specifics can vary, but typically includes mean squared error (MSE) for cycle consistency. | Uses a combination of adversarial loss and cycle consistency loss, often employing L1 norm for the cycle consistency loss. |\n",
        "| **Architecture Specifics** | May vary more in practice, with some implementations focusing on specific types of data or transformations. | Typically follows the architecture as proposed in the original CycleGAN paper, which includes the use of instance normalization and residual blocks. |\n",
        "| **Training Stability** | Often requires careful tuning and may be less stable due to its focus on style transfer, which can introduce more variability. | Generally stable and well-documented, with extensive resources and implementations available. |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ad9777",
      "metadata": {
        "id": "86ad9777"
      },
      "source": [
        "## Example Code Snippets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d43f8cce",
      "metadata": {
        "id": "d43f8cce"
      },
      "source": [
        "### DiscoGAN Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e0305c",
      "metadata": {
        "id": "a8e0305c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Activation, Add, BatchNormalization, LeakyReLU, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    res = Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    res = BatchNormalization()(res)\n",
        "    res = Activation('relu')(res)\n",
        "    res = Conv2D(filters, (3, 3), padding='same')(res)\n",
        "    res = BatchNormalization()(res)\n",
        "    return Add()([res, x])\n",
        "\n",
        "def build_discogan_generator():\n",
        "    inputs = Input(shape=(64, 64, 3))\n",
        "    x = Conv2D(64, (7, 7), padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(128, (3, 3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (3, 3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    for _ in range(6):\n",
        "        x = residual_block(x, 256)\n",
        "    x = Conv2DTranspose(128, (3, 3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(3, (7, 7), padding='same')(x)\n",
        "    x = Activation('tanh')(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "def build_discogan_discriminator():\n",
        "    inputs = Input(shape=(64, 64, 3))\n",
        "    x = Conv2D(64, (4, 4), strides=2, padding='same')(inputs)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(512, (4, 4), strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "def build_discogan(generator_A_to_B, generator_B_to_A, discriminator_A, discriminator_B):\n",
        "    discriminator_A.trainable = False\n",
        "    discriminator_B.trainable = False\n",
        "    real_A = Input(shape=(64, 64, 3))\n",
        "    real_B = Input(shape=(64, 64, 3))\n",
        "    fake_B = generator_A_to_B(real_A)\n",
        "    fake_A = generator_B_to_A(real_B)\n",
        "    recon_A = generator_B_to_A(fake_B)\n",
        "    recon_B = generator_A_to_B(fake_A)\n",
        "    valid_A = discriminator_A(fake_A)\n",
        "    valid_B = discriminator_B(fake_B)\n",
        "    combined = Model(inputs=[real_A, real_B], outputs=[valid_A, valid_B, recon_A, recon_B])\n",
        "    combined.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss=['mse', 'mse', 'mae', 'mae'], loss_weights=[1, 1, 10, 10])\n",
        "    return combined\n",
        "\n",
        "# Instantiate the generators and discriminators\n",
        "generator_A_to_B = build_discogan_generator()\n",
        "generator_B_to_A = build_discogan_generator()\n",
        "discriminator_A = build_discogan_discriminator()\n",
        "discriminator_B = build_discogan_discriminator()\n",
        "\n",
        "# Compile the discriminators\n",
        "discriminator_A.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='mse', metrics=['accuracy'])\n",
        "discriminator_B.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Instantiate and summarize the DiscoGAN\n",
        "discogan = build_discogan(generator_A_to_B, generator_B_to_A, discriminator_A, discriminator_B)\n",
        "discogan.summary()\n",
        "\n",
        "# Sample data\n",
        "data_A = np.random.rand(100, 64, 64, 3).astype(np.float32)\n",
        "data_B = np.random.rand(100, 64, 64, 3).astype(np.float32)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10000\n",
        "batch_size = 1\n",
        "patch_size = discriminator_A.output_shape[1]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for _ in range(data_A.shape[0] // batch_size):\n",
        "        # Train discriminators with real samples\n",
        "        idx = np.random.randint(0, data_A.shape[0], batch_size)\n",
        "        X_real_A, y_real_A = data_A[idx], np.ones((batch_size, patch_size, patch_size, 1))\n",
        "        X_real_B, y_real_B = data_B[idx], np.ones((batch_size, patch_size, patch_size, 1))\n",
        "        dA_loss_real = discriminator_A.train_on_batch(X_real_A, y_real_A)\n",
        "        dB_loss_real = discriminator_B.train_on_batch(X_real_B, y_real_B)\n",
        "\n",
        "        # Train discriminators with fake samples\n",
        "        X_fake_A, y_fake_A = generator_B_to_A.predict(X_real_B), np.zeros((batch_size, patch_size, patch_size, 1))\n",
        "        X_fake_B, y_fake_B = generator_A_to_B.predict(X_real_A), np.zeros((batch_size, patch_size, patch_size, 1))\n",
        "        dA_loss_fake = discriminator_A.train_on_batch(X_fake_A, y_fake_A)\n",
        "        dB_loss_fake = discriminator_B.train_on_batch(X_fake_B, y_fake_B)\n",
        "\n",
        "        # Train generators\n",
        "        g_loss = discogan.train_on_batch([X_real_A, X_real_B], [y_real_A, y_real_B, X_real_A, X_real_B])\n",
        "\n",
        "    # Summarize the loss for this epoch\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, dA_real_loss={dA_loss_real[0]}, dA_fake_loss={dA_loss_fake[0]}, dB_real_loss={dB_loss_real[0]}, dB_fake_loss={dB_loss_fake[0]}, g_loss={g_loss[0]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f771919f",
      "metadata": {
        "id": "f771919f"
      },
      "source": [
        "### CycleGAN Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13168038",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "13168038",
        "outputId": "053c2aa1-656f-4f21-98fb-57ced9609cb2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_4             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │      \u001b[38;5;34m7,845,123\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │      \u001b[38;5;34m7,845,123\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_2 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │      \u001b[38;5;34m2,766,529\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_3 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │      \u001b[38;5;34m2,766,529\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_4             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,845,123</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,845,123</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,766,529</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,766,529</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,223,304\u001b[0m (80.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,223,304</span> (80.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,690,246\u001b[0m (59.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,690,246</span> (59.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,533,058\u001b[0m (21.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,533,058</span> (21.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x78d28ee50f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x78d28ef7f0a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Activation, Add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class InstanceNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        super(InstanceNormalization, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer=\"ones\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
        "        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
        "        return self.gamma * normalized + self.beta\n",
        "\n",
        "# Define the residual block\n",
        "def residual_block(x, filters):\n",
        "    res = Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    res = InstanceNormalization()(res)\n",
        "    res = Activation('relu')(res)\n",
        "    res = Conv2D(filters, (3, 3), padding='same')(res)\n",
        "    res = InstanceNormalization()(res)\n",
        "    return Add()([res, x])\n",
        "\n",
        "# Function to build the generator\n",
        "def build_generator():\n",
        "    inputs = Input(shape=(64, 64, 3))\n",
        "    x = Conv2D(64, (7, 7), padding='same')(inputs)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(128, (3, 3), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (3, 3), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    for _ in range(6):\n",
        "        x = residual_block(x, 256)\n",
        "    x = Conv2DTranspose(128, (3, 3), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(3, (7, 7), padding='same')(x)\n",
        "    x = Activation('tanh')(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Function to build the discriminator\n",
        "def build_discriminator():\n",
        "    inputs = Input(shape=(64, 64, 3))\n",
        "    x = Conv2D(64, (4, 4), strides=2, padding='same')(inputs)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(512, (4, 4), strides=2, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Function to build the combined DiscoGAN model\n",
        "def build_discogan(generator_A_to_B, generator_B_to_A, discriminator_A, discriminator_B):\n",
        "    discriminator_A.trainable = False\n",
        "    discriminator_B.trainable = False\n",
        "    real_A = Input(shape=(64, 64, 3))\n",
        "    real_B = Input(shape=(64, 64, 3))\n",
        "    fake_B = generator_A_to_B(real_A)\n",
        "    fake_A = generator_B_to_A(real_B)\n",
        "    recon_A = generator_B_to_A(fake_B)\n",
        "    recon_B = generator_A_to_B(fake_A)\n",
        "    valid_A = discriminator_A(fake_A)\n",
        "    valid_B = discriminator_B(fake_B)\n",
        "    combined = Model(inputs=[real_A, real_B], outputs=[valid_A, valid_B, recon_A, recon_B])\n",
        "    combined.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss=['mse', 'mse', 'mae', 'mae'], loss_weights=[1, 1, 10, 10])\n",
        "    return combined\n",
        "\n",
        "# Instantiate the generators and discriminators\n",
        "generator_A_to_B = build_generator()\n",
        "generator_B_to_A = build_generator()\n",
        "discriminator_A = build_discriminator()\n",
        "discriminator_B = build_discriminator()\n",
        "\n",
        "# Compile the discriminators\n",
        "discriminator_A.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='mse', metrics=['accuracy'])\n",
        "discriminator_B.compile(optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), loss='mse', metrics=['accuracy'])\n",
        "\n",
        "# Instantiate and summarize the DiscoGAN\n",
        "discogan = build_discogan(generator_A_to_B, generator_B_to_A, discriminator_A, discriminator_B)\n",
        "discogan.summary()\n",
        "\n",
        "# Sample data\n",
        "data_A = np.random.rand(100, 64, 64, 3).astype(np.float32)\n",
        "data_B = np.random.rand(100, 64, 64, 3).astype(np.float32)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10000\n",
        "batch_size = 1\n",
        "patch_size = discriminator_A.output_shape[1]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for _ in range(data_A.shape[0] // batch_size):\n",
        "        # Train discriminators with real samples\n",
        "        idx = np.random.randint(0, data_A.shape[0], batch_size)\n",
        "        X_real_A, y_real_A = data_A[idx], np.ones((batch_size, patch_size, patch_size, 1))\n",
        "        X_real_B, y_real_B = data_B[idx], np.ones((batch_size, patch_size, patch_size, 1))\n",
        "        dA_loss_real = discriminator_A.train_on_batch(X_real_A, y_real_A)\n",
        "        dB_loss_real = discriminator_B.train_on_batch(X_real_B, y_real_B)\n",
        "\n",
        "        # Train discriminators with fake samples\n",
        "        X_fake_A, y_fake_A = generator_B_to_A.predict(X_real_B), np.zeros((batch_size, patch_size, patch_size, 1))\n",
        "        X_fake_B, y_fake_B = generator_A_to_B.predict(X_real_A), np.zeros((batch_size, patch_size, patch_size, 1))\n",
        "        dA_loss_fake = discriminator_A.train_on_batch(X_fake_A, y_fake_A)\n",
        "        dB_loss_fake = discriminator_B.train_on_batch(X_fake_B, y_fake_B)\n",
        "\n",
        "        # Train generators\n",
        "        g_loss = discogan.train_on_batch([X_real_A, X_real_B], [y_real_A, y_real_B, X_real_A, X_real_B])\n",
        "\n",
        "    # Summarize the loss for this epoch\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs}, dA_real_loss={dA_loss_real[0]}, dA_fake_loss={dA_loss_fake[0]}, dB_real_loss={dB_loss_real[0]}, dB_fake_loss={dB_loss_fake[0]}, g_loss={g_loss[0]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0f32f8",
      "metadata": {
        "id": "3e0f32f8"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e314f9",
      "metadata": {
        "id": "46e314f9"
      },
      "source": [
        "Both DiscoGAN and CycleGAN are powerful tools for unsupervised image-to-image translation, with each having its own strengths and areas of application. Understanding their differences can help you choose the right model for your specific task."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}