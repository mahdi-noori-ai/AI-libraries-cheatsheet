{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad37eec9",
   "metadata": {},
   "source": [
    "\n",
    "# Attention U-Net: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Attention U-Net, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66478dbe",
   "metadata": {},
   "source": [
    "\n",
    "## History of Attention U-Net\n",
    "\n",
    "Attention U-Net was introduced by Oktay et al. in 2018 in the paper \"Attention U-Net: Learning Where to Look for the Pancreas.\" This model builds upon the original U-Net architecture by incorporating attention mechanisms into the network. The attention gates allow the network to focus on the most relevant regions of the image, improving segmentation accuracy, especially in cases where the objects of interest vary in shape and size or are located in complex backgrounds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdead609",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Attention U-Net\n",
    "\n",
    "### Attention U-Net Architecture\n",
    "\n",
    "Attention U-Net extends the original U-Net architecture by introducing attention gates in the skip connections between the encoder and decoder. These attention gates help the network focus on important regions of the input image while suppressing irrelevant information.\n",
    "\n",
    "1. **Attention Gate (AG)**: The attention gate is a key component of the Attention U-Net. It selectively highlights important features in the encoder path before they are merged with the decoder path. The gate takes both the feature map from the encoder and the corresponding decoder output as inputs and generates an attention coefficient.\n",
    "\n",
    "\\[\n",
    "\\alpha_i = \\sigma(w_x^T x_i + w_g^T g + b)\n",
    "\\]\n",
    "\n",
    "Where \\( x_i \\) is the input feature map from the encoder, \\( g \\) is the gating signal from the decoder, and \\( \\alpha_i \\) is the attention coefficient, computed using a sigmoid activation function \\( \\sigma \\).\n",
    "\n",
    "2. **Attention-Weighted Feature Map**: The attention coefficient \\( \\alpha_i \\) is used to scale the input feature map, enhancing the important features while suppressing irrelevant ones.\n",
    "\n",
    "\\[\n",
    "\\hat{x}_i = \\alpha_i \\cdot x_i\n",
    "\\]\n",
    "\n",
    "Where \\( \\hat{x}_i \\) is the attention-weighted feature map.\n",
    "\n",
    "3. **Skip Connections**: The attention-weighted feature map \\( \\hat{x}_i \\) is then passed to the decoder through skip connections, helping the network retain spatial information while focusing on relevant regions.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Attention U-Net typically uses the Dice coefficient loss for training, similar to the original U-Net, which is effective for handling imbalanced datasets.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{Dice}} = 1 - \\frac{2 \\sum_i p_i y_i + \\epsilon}{\\sum_i p_i + \\sum_i y_i + \\epsilon}\n",
    "\\]\n",
    "\n",
    "Where \\( p_i \\) is the predicted probability, \\( y_i \\) is the ground truth label, and \\( \\epsilon \\) is a small constant to avoid division by zero.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training Attention U-Net involves optimizing the Dice coefficient loss using backpropagation and stochastic gradient descent (SGD) or its variants. The attention gates allow the network to focus on the most relevant parts of the image, improving segmentation accuracy, particularly in challenging scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f321c7",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a simplified version of Attention U-Net using TensorFlow and Keras. This implementation will demonstrate the core concepts of Attention U-Net, including the use of attention gates in the skip connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837631af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def attention_gate(x, g, inter_shape):\n",
    "    theta_x = layers.Conv2D(inter_shape, (1, 1), padding='same')(x)\n",
    "    phi_g = layers.Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    add_xg = layers.add([theta_x, phi_g])\n",
    "    relu_xg = layers.Activation('relu')(add_xg)\n",
    "    psi = layers.Conv2D(1, (1, 1), padding='same')(relu_xg)\n",
    "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
    "    return layers.multiply([x, sigmoid_xg])\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n",
    "    x = layers.Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def attention_unet(input_shape, num_classes, filters=64):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    e1 = conv_block(inputs, filters)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(e1)\n",
    "    \n",
    "    e2 = conv_block(p1, filters*2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(e2)\n",
    "    \n",
    "    e3 = conv_block(p2, filters*4)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(e3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = conv_block(p3, filters*8)\n",
    "    \n",
    "    # Decoder with Attention Gates\n",
    "    g3 = conv_block(b, filters*4)\n",
    "    a3 = attention_gate(e3, g3, filters*4)\n",
    "    d3 = layers.Concatenate()([a3, g3])\n",
    "    d3 = conv_block(d3, filters*4)\n",
    "    \n",
    "    g2 = conv_block(d3, filters*2)\n",
    "    a2 = attention_gate(e2, g2, filters*2)\n",
    "    d2 = layers.Concatenate()([a2, g2])\n",
    "    d2 = conv_block(d2, filters*2)\n",
    "    \n",
    "    g1 = conv_block(d2, filters)\n",
    "    a1 = attention_gate(e1, g1, filters)\n",
    "    d1 = layers.Concatenate()([a1, g1])\n",
    "    d1 = conv_block(d1, filters)\n",
    "    \n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(d1)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = 3  # Example number of classes\n",
    "model = attention_unet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x_train = np.random.rand(10, 128, 128, 3)\n",
    "y_train = np.random.randint(0, num_classes, (10, 128, 128, 1))\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=2)\n",
    "\n",
    "# Plot training accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073add41",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Attention U-Net\n",
    "\n",
    "### Advantages\n",
    "- **Enhanced Focus**: The attention gates allow the model to focus on the most relevant regions in the image, improving segmentation accuracy, especially in complex scenarios.\n",
    "- **Better Generalization**: By focusing on important regions, Attention U-Net tends to generalize better on unseen data, reducing the risk of overfitting.\n",
    "\n",
    "### Disadvantages\n",
    "- **Increased Computational Cost**: The addition of attention gates increases the computational complexity, leading to longer training times and higher memory usage.\n",
    "- **Complexity in Implementation**: The integration of attention mechanisms adds to the architectural complexity, making it more challenging to implement and tune.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a99ab",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Attention U-Net extends the original U-Net architecture by incorporating attention mechanisms, allowing the model to focus on the most relevant parts of the image. This enhancement leads to improved segmentation accuracy, particularly in challenging scenarios. While the addition of attention gates increases the model's complexity and computational requirements, the benefits in terms of accuracy and generalization often outweigh these costs. Attention U-Net is particularly well-suited for tasks where ob...\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
