{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2b04e6",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Word2Vec, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db97ba2",
   "metadata": {},
   "source": [
    "\n",
    "## History of Word2Vec\n",
    "\n",
    "Word2Vec was introduced by Tomas Mikolov and colleagues at Google in 2013 in the papers \"Efficient Estimation of Word Representations in Vector Space\" and \"Distributed Representations of Words and Phrases and their Compositionality.\" The model revolutionized natural language processing (NLP) by providing a method to represent words as continuous vectors in a high-dimensional space, capturing semantic relationships between words based on their context in large text corpora. Word2Vec laid the foundation fo...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741dd2f",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Word2Vec\n",
    "\n",
    "### Skip-Gram Model\n",
    "\n",
    "One of the two architectures in Word2Vec is the Skip-Gram model, which predicts the context words given a target word. Given a sequence of training words \\( w_1, w_2, \\dots, w_T \\), the Skip-Gram model aims to maximize the following average log probability:\n",
    "\n",
    "\\[\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)\n",
    "\\]\n",
    "\n",
    "Where \\( c \\) is the context window size, \\( w_t \\) is the target word, and \\( w_{t+j} \\) are the context words.\n",
    "\n",
    "### CBOW Model\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model, the second architecture in Word2Vec, works in the opposite manner: it predicts the target word given the context words. The objective function for CBOW is to maximize the probability of the target word given the context:\n",
    "\n",
    "\\[\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} \\log p(w_t | w_{t-c}, \\dots, w_{t+c})\n",
    "\\]\n",
    "\n",
    "### Softmax and Negative Sampling\n",
    "\n",
    "The output of the Skip-Gram or CBOW model is typically passed through a softmax function to calculate the probability distribution over the vocabulary:\n",
    "\n",
    "\\[\n",
    "p(w_O | w_I) = \\frac{\\exp(v_{w_O}^\\top v_{w_I})}{\\sum_{w=1}^{|V|} \\exp(v_w^\\top v_{w_I})}\n",
    "\\]\n",
    "\n",
    "Where \\( v_{w_O} \\) and \\( v_{w_I} \\) are the vectors for the output and input words, respectively, and \\( |V| \\) is the size of the vocabulary.\n",
    "\n",
    "To efficiently train the model, especially with large vocabularies, Word2Vec uses techniques like Negative Sampling and Hierarchical Softmax. Negative Sampling, for instance, approximates the softmax function by only updating a small sample of negative examples instead of the entire vocabulary.\n",
    "\n",
    "\\[\n",
    "\\log \\sigma(v_{w_O}^\\top v_{w_I}) + \\sum_{i=1}^k \\mathbb{E}_{w_i \\sim P_n(w)}[\\log \\sigma(-v_{w_i}^\\top v_{w_I})]\n",
    "\\]\n",
    "\n",
    "Where \\( \\sigma \\) is the sigmoid function, \\( k \\) is the number of negative samples, and \\( P_n(w) \\) is the noise distribution.\n",
    "\n",
    "### Training\n",
    "\n",
    "Word2Vec is trained using stochastic gradient descent (SGD) or its variants. The model learns to adjust the word vectors such that words appearing in similar contexts have similar vector representations, capturing semantic relationships like \"king\" - \"man\" + \"woman\" = \"queen\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639c2df",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of Word2Vec using the Gensim library. This implementation will demonstrate the key concepts of Word2Vec, including training a Skip-Gram model on a sample corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\"],\n",
    "    [\"king\", \"man\", \"kingdom\"],\n",
    "    [\"queen\", \"woman\", \"monarchy\"],\n",
    "    [\"man\", \"woman\", \"child\"],\n",
    "    [\"woman\", \"queen\", \"lady\"],\n",
    "    [\"man\", \"king\", \"lord\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Print word vectors\n",
    "word_vectors = model.wv\n",
    "print(\"Vector for 'king':\", word_vectors['king'])\n",
    "print(\"Vector for 'queen':\", word_vectors['queen'])\n",
    "\n",
    "# Plot word vectors using PCA\n",
    "words = list(word_vectors.index_to_key)\n",
    "vectors = [word_vectors[word] for word in words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.title('Word2Vec Word Embeddings')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a9d68",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Word2Vec\n",
    "\n",
    "### Advantages\n",
    "- **Efficient Learning**: Word2Vec efficiently captures semantic relationships between words, making it a powerful tool for various NLP tasks.\n",
    "- **Low Dimensionality**: The word vectors generated by Word2Vec are typically low-dimensional, which makes them computationally efficient for downstream tasks.\n",
    "- **Versatility**: Word2Vec can be used for various tasks such as finding word similarities, clustering words, and improving the performance of machine learning models in NLP.\n",
    "\n",
    "### Disadvantages\n",
    "- **Context Independence**: Word2Vec does not capture the context of words in different sentences, which can lead to a lack of understanding of polysemy (words with multiple meanings).\n",
    "- **Memory Usage**: Training Word2Vec on large corpora requires significant memory, especially when using large vocabularies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa2cbb",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Word2Vec marked a significant milestone in the field of natural language processing by providing a method to efficiently represent words as vectors in a continuous space. These word vectors capture semantic relationships between words, enabling various downstream tasks in NLP. While Word2Vec has some limitations, such as context independence, its impact on the field is undeniable, and it continues to be widely used in many applications.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
