{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b1482a",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Convolutional Network (GCN): A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Graph Convolutional Networks (GCNs), including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab5b13",
   "metadata": {},
   "source": [
    "\n",
    "## History of Graph Convolutional Networks (GCNs)\n",
    "\n",
    "Graph Convolutional Networks (GCNs) were introduced by Thomas Kipf and Max Welling in their 2016 paper \"Semi-Supervised Classification with Graph Convolutional Networks.\" The concept of applying convolutional operations to graph data was a significant breakthrough in the field of graph-based machine learning. GCNs extended the success of convolutional neural networks (CNNs) in processing grid-like data (e.g., images) to more general graph-structured data. Since their introduction, GCNs have been widely ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4571dc",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Graph Convolutional Networks\n",
    "\n",
    "### Graph Representation\n",
    "\n",
    "A graph \\( G = (V, E) \\) consists of a set of nodes \\( V \\) and edges \\( E \\) connecting them. The graph can be represented by an adjacency matrix \\( A \\), where \\( A_{ij} = 1 \\) if there is an edge between nodes \\( i \\) and \\( j \\), and \\( 0 \\) otherwise. Each node \\( v_i \\) is associated with a feature vector \\( x_i \\).\n",
    "\n",
    "### Graph Convolution Operation\n",
    "\n",
    "The core idea of GCNs is to apply convolutional operations on graphs, where the convolution is performed over the graph's structure. The graph convolution operation for a node \\( v_i \\) is defined as:\n",
    "\n",
    "\\[\n",
    "h_i^{(l+1)} = \\sigma \\left( \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{c_{ij}} W^{(l)} h_j^{(l)} + b^{(l)} \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( h_i^{(l+1)} \\) is the hidden state of node \\( i \\) at layer \\( l+1 \\).\n",
    "- \\( \\mathcal{N}(i) \\) represents the set of neighbors of node \\( i \\).\n",
    "- \\( W^{(l)} \\) is the weight matrix at layer \\( l \\).\n",
    "- \\( b^{(l)} \\) is the bias term at layer \\( l \\).\n",
    "- \\( \\sigma \\) is the activation function (e.g., ReLU).\n",
    "- \\( c_{ij} \\) is a normalization constant, often chosen as \\( c_{ij} = \\sqrt{\\text{deg}(i) \\cdot \\text{deg}(j)} \\), where \\( \\text{deg}(i) \\) is the degree of node \\( i \\).\n",
    "\n",
    "### Layer-wise Propagation Rule\n",
    "\n",
    "The layer-wise propagation rule for GCNs can be compactly written as:\n",
    "\n",
    "\\[\n",
    "H^{(l+1)} = \\sigma \\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)} \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\tilde{A} = A + I \\) is the adjacency matrix with added self-loops (identity matrix \\( I \\)).\n",
    "- \\( \\tilde{D} \\) is the degree matrix of \\( \\tilde{A} \\).\n",
    "- \\( H^{(l)} \\) is the matrix of node features at layer \\( l \\).\n",
    "- \\( W^{(l)} \\) is the weight matrix at layer \\( l \\).\n",
    "- \\( \\sigma \\) is the activation function.\n",
    "\n",
    "### Final Layer\n",
    "\n",
    "In a typical GCN used for node classification, the final layer is a softmax function that outputs a probability distribution over the possible classes for each node:\n",
    "\n",
    "\\[\n",
    "Z = \\text{softmax} \\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(L-1)} W^{(L)} \\right)\n",
    "\\]\n",
    "\n",
    "Where \\( Z \\) is the matrix of predicted class probabilities for each node.\n",
    "\n",
    "### Training\n",
    "\n",
    "GCNs are trained using gradient-based optimization techniques, with the cross-entropy loss function being commonly used for node classification tasks:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_L} y_i \\log(Z_i)\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathcal{V}_L \\) is the set of labeled nodes, \\( y_i \\) is the true label, and \\( Z_i \\) is the predicted probability for node \\( i \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cbd92",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of a Graph Convolutional Network (GCN) using TensorFlow and Keras. This implementation will demonstrate how to build a GCN for node classification on a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f01a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "class GraphConvolution(layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[1][-1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, adjacency = inputs\n",
    "        x = tf.matmul(adjacency, x)\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        return x\n",
    "\n",
    "def build_gcn(input_dim, hidden_dim, output_dim, num_nodes):\n",
    "    adjacency = layers.Input(shape=(num_nodes,), sparse=True)\n",
    "    features = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    x = GraphConvolution(hidden_dim)([features, adjacency])\n",
    "    x = layers.ReLU()(x)\n",
    "    x = GraphConvolution(output_dim)([x, adjacency])\n",
    "    outputs = layers.Softmax()(x)\n",
    "    \n",
    "    model = models.Model(inputs=[features, adjacency], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "input_dim = 10   # Example input feature dimension\n",
    "hidden_dim = 16  # Number of hidden units\n",
    "output_dim = 3   # Number of output classes\n",
    "num_nodes = 100  # Number of nodes in the graph\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_gcn(input_dim, hidden_dim, output_dim, num_nodes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x_train = np.random.rand(num_nodes, input_dim)\n",
    "adjacency = np.random.rand(num_nodes, num_nodes)\n",
    "adjacency = (adjacency + adjacency.T) / 2  # Make adjacency symmetric\n",
    "adjacency[adjacency < 0.5] = 0  # Sparsify\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randint(output_dim, size=(num_nodes,)))\n",
    "\n",
    "# Train the model\n",
    "model.fit([x_train, adjacency], y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018f218",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Graph Convolutional Networks (GCNs)\n",
    "\n",
    "### Advantages\n",
    "- **Captures Graph Structure**: GCNs are designed to effectively capture the structural information of graphs, making them well-suited for tasks like node classification and link prediction.\n",
    "- **Versatility**: GCNs can be applied to a wide range of graph-based tasks across different domains, including social networks, biological networks, and recommendation systems.\n",
    "- **Scalability**: With the right optimizations, GCNs can scale to large graphs, making them practical for real-world applications.\n",
    "\n",
    "### Disadvantages\n",
    "- **Over-smoothing**: As the number of GCN layers increases, the representations of nodes can become overly similar, leading to a loss of discriminative power.\n",
    "- **Limited Expressiveness**: GCNs may struggle to capture complex graph structures, particularly in graphs with heterophily (where connected nodes have different labels).\n",
    "- **Computational Complexity**: The need to compute graph convolutions on large adjacency matrices can be computationally expensive, especially for very large graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115086a",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Graph Convolutional Networks (GCNs) have emerged as a powerful tool for processing graph-structured data, offering significant advantages in capturing the inherent structure of graphs. They have been successfully applied to a variety of tasks, including node classification, link prediction, and graph classification. However, GCNs also face challenges, such as over-smoothing and computational complexity, which researchers continue to address. Despite these challenges, GCNs remain a key model in the graph...\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
