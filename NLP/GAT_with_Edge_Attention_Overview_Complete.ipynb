{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dfe32d3",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Attention Network with Edge Attention: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Graph Attention Networks with Edge Attention (GAT with Edge Attention), including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57c4fa",
   "metadata": {},
   "source": [
    "\n",
    "## History of Graph Attention Networks with Edge Attention\n",
    "\n",
    "Graph Attention Networks (GATs) were introduced by Petar Veličković et al. in 2017, bringing attention mechanisms to graph neural networks. While GATs allow for node-wise attention, the idea of integrating edge attributes or attentions into the GAT framework led to the development of variants such as Graph Attention Networks with Edge Attention. This variant enables the model to consider both node and edge features in the attention mechanism, allowing for a more nuanced understanding of the graph's stru...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d72a5a",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of GAT with Edge Attention\n",
    "\n",
    "### Edge Attention Mechanism\n",
    "\n",
    "The primary innovation in GAT with Edge Attention is the incorporation of edge features into the attention mechanism. This allows the model to not only weigh the importance of neighboring nodes but also to consider the strength or type of connections between them.\n",
    "\n",
    "1. **Edge Feature Incorporation**: Given an edge feature vector \\(e_{ij}\\) for the edge between nodes \\(i\\) and \\(j\\), the attention score is computed as:\n",
    "\n",
    "\\[\n",
    "e_{ij} = \\text{LeakyReLU}(\\mathbf{a}^\\top [\\mathbf{W} h_i \\, \\| \\, \\mathbf{W} h_j \\, \\| \\, \\mathbf{W_e} e_{ij}])\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\mathbf{W}\\) is the weight matrix for node features.\n",
    "- \\(\\mathbf{W_e}\\) is the weight matrix for edge features.\n",
    "- \\(\\mathbf{a}\\) is the attention vector.\n",
    "- \\(\\|\\) denotes concatenation.\n",
    "- \\(e_{ij}\\) is the edge feature.\n",
    "\n",
    "2. **Attention Coefficient Calculation**: The attention coefficients \\(\\alpha_{ij}\\) are computed using a softmax function, similar to traditional GAT:\n",
    "\n",
    "\\[\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n",
    "\\]\n",
    "\n",
    "3. **Aggregation with Edge Attention**: The final node representation incorporates the edge-attended neighbor features:\n",
    "\n",
    "\\[\n",
    "h_i' = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W} h_j\\right)\n",
    "\\]\n",
    "\n",
    "Where \\(\\sigma\\) is a non-linear activation function, such as ReLU.\n",
    "\n",
    "### Multi-Head Edge Attention\n",
    "\n",
    "As with the standard GAT, multi-head attention can be employed to stabilize the learning process and capture more complex interactions:\n",
    "\n",
    "\\[\n",
    "h_i' = \\|_{k=1}^{K} \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(k)} \\mathbf{W}^{(k)} h_j\\right)\n",
    "\\]\n",
    "\n",
    "Where \\(K\\) is the number of attention heads, and each head may attend to different aspects of the node and edge features.\n",
    "\n",
    "### Final Layer\n",
    "\n",
    "For tasks like node classification, the final layer typically involves a softmax function to output probabilities over the possible classes for each node:\n",
    "\n",
    "\\[\n",
    "Z = \\text{softmax}(H')\n",
    "\\]\n",
    "\n",
    "Where \\(Z\\) is the matrix of predicted class probabilities for each node.\n",
    "\n",
    "### Training\n",
    "\n",
    "The model is trained using gradient-based optimization, with the cross-entropy loss function commonly used for node classification:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_L} y_i \\log(Z_i)\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathcal{V}_L \\) is the set of labeled nodes, \\( y_i \\) is the true label, and \\( Z_i \\) is the predicted probability for node \\( i \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21368a4",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of a Graph Attention Network with Edge Attention using TensorFlow and Keras. This implementation will demonstrate how to build a GAT with Edge Attention for node classification on a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "class EdgeAttentionLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=1, **kwargs):\n",
    "        super(EdgeAttentionLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = [self.add_weight(shape=(2 * output_dim + output_dim, 1), initializer='glorot_uniform', trainable=True) for _ in range(num_heads)]\n",
    "        self.kernel = self.add_weight(shape=(output_dim, output_dim), initializer='glorot_uniform', trainable=True)\n",
    "        self.edge_kernel = self.add_weight(shape=(output_dim, output_dim), initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, adjacency, edge_features = inputs\n",
    "        features = tf.matmul(x, self.kernel)\n",
    "        edge_features = tf.matmul(edge_features, self.edge_kernel)\n",
    "        outputs = []\n",
    "\n",
    "        for head in self.attention_heads:\n",
    "            attn_coeffs = []\n",
    "            for i in range(features.shape[0]):\n",
    "                e_ij = tf.reduce_sum(tf.nn.leaky_relu(tf.matmul(tf.concat([features[i], features, edge_features], axis=-1), head)), axis=1)\n",
    "                attention = tf.nn.softmax(e_ij, axis=0)\n",
    "                attn_coeffs.append(attention)\n",
    "\n",
    "            attn_coeffs = tf.stack(attn_coeffs)\n",
    "            h_prime = tf.matmul(attn_coeffs, features)\n",
    "            outputs.append(h_prime)\n",
    "\n",
    "        output = tf.concat(outputs, axis=-1) if self.num_heads > 1 else outputs[0]\n",
    "        return output\n",
    "\n",
    "def build_gat_edge_attention(input_dim, output_dim, edge_dim, num_heads, num_nodes):\n",
    "    adjacency = layers.Input(shape=(num_nodes,), sparse=True)\n",
    "    features = layers.Input(shape=(input_dim,))\n",
    "    edge_features = layers.Input(shape=(num_nodes, edge_dim))\n",
    "    \n",
    "    x = EdgeAttentionLayer(output_dim, num_heads)([features, adjacency, edge_features])\n",
    "    x = layers.ReLU()(x)\n",
    "    x = EdgeAttentionLayer(output_dim, num_heads)([x, adjacency, edge_features])\n",
    "    outputs = layers.Softmax()(x)\n",
    "    \n",
    "    model = models.Model(inputs=[features, adjacency, edge_features], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "input_dim = 10   # Example input feature dimension\n",
    "output_dim = 3   # Number of output classes\n",
    "edge_dim = 5     # Dimension of edge features\n",
    "num_heads = 8    # Number of attention heads\n",
    "num_nodes = 100  # Number of nodes in the graph\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_gat_edge_attention(input_dim, output_dim, edge_dim, num_heads, num_nodes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x_train = np.random.rand(num_nodes, input_dim)\n",
    "adjacency = np.random.rand(num_nodes, num_nodes)\n",
    "adjacency = (adjacency + adjacency.T) / 2  # Make adjacency symmetric\n",
    "adjacency[adjacency < 0.5] = 0  # Sparsify\n",
    "edge_features = np.random.rand(num_nodes, num_nodes, edge_dim)\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randint(output_dim, size=(num_nodes,)))\n",
    "\n",
    "# Train the model\n",
    "model.fit([x_train, adjacency, edge_features], y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a2f70",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Graph Attention Networks with Edge Attention\n",
    "\n",
    "### Advantages\n",
    "- **Enhanced Expressiveness**: By incorporating edge features into the attention mechanism, GAT with Edge Attention can capture more complex relationships between nodes, making the model more expressive.\n",
    "- **Applicability to Edge-Attributed Graphs**: This model is particularly useful for graphs where edge attributes play a significant role, such as social networks, molecular graphs, and transportation networks.\n",
    "- **Improved Performance**: The additional edge attention often leads to improved performance on tasks where the edge features are important for understanding the graph structure.\n",
    "\n",
    "### Disadvantages\n",
    "- **Increased Computational Complexity**: Incorporating edge features into the attention mechanism increases the model's complexity and computational cost, especially for large graphs.\n",
    "- **Overfitting Risk**: The model's increased capacity may lead to overfitting, particularly on small datasets with noisy edge features.\n",
    "- **Complexity in Implementation**: The addition of edge attention requires more careful tuning and can be more challenging to implement and debug compared to standard GATs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2292ad",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Graph Attention Networks with Edge Attention represent a significant advancement in the field of graph neural networks by introducing the ability to consider both node and edge features in the attention mechanism. This capability allows the model to capture more complex relationships within the graph, leading to improved performance on tasks where edge attributes are crucial. However, the increased complexity and computational cost present challenges that need to be carefully managed. Despite these chall...\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
