{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff612a6c",
   "metadata": {},
   "source": [
    "\n",
    "# GloVe: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of GloVe (Global Vectors for Word Representation), including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b280b1",
   "metadata": {},
   "source": [
    "\n",
    "## History of GloVe\n",
    "\n",
    "GloVe was introduced by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford University in 2014 in the paper \"GloVe: Global Vectors for Word Representation.\" The model was developed to address some of the limitations of earlier word embedding models like Word2Vec. GloVe differs from Word2Vec by incorporating global word co-occurrence statistics from a corpus, allowing it to capture a more nuanced understanding of word relationships. GloVe has since become one of the most widely used w...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38474bcb",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of GloVe\n",
    "\n",
    "### Co-occurrence Matrix\n",
    "\n",
    "GloVe is based on the idea of constructing a word co-occurrence matrix from a large corpus, where each entry \\(X_{ij}\\) in the matrix represents the number of times word \\(i\\) occurs in the context of word \\(j\\). The co-occurrence matrix captures the statistical information about word occurrences across the entire corpus.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "The key insight of GloVe is that the ratio of co-occurrence probabilities between pairs of words can be used to encode meaningful word relationships. The model defines a weighted least squares objective function to learn word vectors that capture these relationships:\n",
    "\n",
    "\\[\n",
    "J = \\sum_{i,j=1}^{|V|} f(X_{ij}) \\left( w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{ij}\\) is the co-occurrence matrix entry for words \\(i\\) and \\(j\\).\n",
    "- \\(w_i\\) and \\(\\tilde{w}_j\\) are the word vectors for words \\(i\\) and \\(j\\).\n",
    "- \\(b_i\\) and \\(\\tilde{b}_j\\) are the bias terms.\n",
    "- \\(f(X_{ij})\\) is a weighting function that reduces the impact of very frequent co-occurrences.\n",
    "\n",
    "### Weighting Function\n",
    "\n",
    "The weighting function \\(f(X_{ij})\\) is designed to give less importance to very frequent co-occurrences, as they tend to be less informative:\n",
    "\n",
    "\\[\n",
    "f(X_{ij}) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\left(\\frac{X_{ij}}{X_{\\max}}\\right)^\\alpha & \\text{if } X_{ij} < X_{\\max} \\\\\n",
    "    1 & \\text{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\]\n",
    "\n",
    "Where \\(X_{\\max}\\) is a threshold and \\(\\alpha\\) is a parameter that controls the scaling.\n",
    "\n",
    "### Training\n",
    "\n",
    "GloVe is trained by minimizing the objective function \\(J\\) using stochastic gradient descent (SGD) or its variants. The resulting word vectors are learned such that words appearing in similar contexts have similar vector representations, capturing semantic relationships between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f751454",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of GloVe using the `glove-python-binary` library. This implementation will demonstrate how to train GloVe embeddings on a sample corpus and visualize the resulting word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5653fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from glove import Corpus, Glove\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\"],\n",
    "    [\"king\", \"man\", \"kingdom\"],\n",
    "    [\"queen\", \"woman\", \"monarchy\"],\n",
    "    [\"man\", \"woman\", \"child\"],\n",
    "    [\"woman\", \"queen\", \"lady\"],\n",
    "    [\"man\", \"king\", \"lord\"]\n",
    "]\n",
    "\n",
    "# Create a corpus object and train the GloVe model\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=2)\n",
    "\n",
    "glove = Glove(no_components=50, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=10, no_threads=1, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "# Print word vectors\n",
    "print(\"Vector for 'king':\", glove.word_vectors[glove.dictionary['king']])\n",
    "print(\"Vector for 'queen':\", glove.word_vectors[glove.dictionary['queen']])\n",
    "\n",
    "# Plot word vectors using PCA\n",
    "words = list(glove.dictionary.keys())\n",
    "vectors = np.array([glove.word_vectors[glove.dictionary[word]] for word in words])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.title('GloVe Word Embeddings')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf94b89",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of GloVe\n",
    "\n",
    "### Advantages\n",
    "- **Global Context**: GloVe captures global word co-occurrence statistics, providing a more comprehensive understanding of word relationships than local context-based models like Word2Vec.\n",
    "- **Efficient Training**: GloVe can be trained on large corpora efficiently, making it suitable for generating high-quality word embeddings for large vocabularies.\n",
    "- **Semantic Relationships**: The embeddings generated by GloVe effectively capture semantic relationships between words, making them useful for various NLP tasks.\n",
    "\n",
    "### Disadvantages\n",
    "- **Memory Usage**: The co-occurrence matrix can be large, especially for extensive vocabularies, leading to high memory usage during training.\n",
    "- **Context Independence**: Like Word2Vec, GloVe does not capture the context in which words appear, leading to limitations in understanding polysemy (words with multiple meanings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a88ca",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "GloVe is a powerful model for generating word embeddings that capture semantic relationships between words by leveraging global word co-occurrence statistics. It has become one of the most widely used methods for word representation in NLP, providing high-quality embeddings that are effective for a variety of tasks. While GloVe has some limitations, such as context independence and memory usage, its impact on the field of NLP is significant, and it remains a valuable tool for many applications.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
