{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bc5f34",
   "metadata": {},
   "source": [
    "\n",
    "# GraphSAGE: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of GraphSAGE, including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef309a",
   "metadata": {},
   "source": [
    "\n",
    "## History of GraphSAGE\n",
    "\n",
    "GraphSAGE (Graph Sample and Aggregate) was introduced by William L. Hamilton, Rex Ying, and Jure Leskovec in their 2017 paper \"Inductive Representation Learning on Large Graphs.\" GraphSAGE was designed to address the limitations of traditional graph convolutional networks (GCNs) that require the entire graph to be present during training. GraphSAGE introduced an inductive learning approach that allows the model to generalize to unseen nodes, making it more scalable and applicable to large, dynamic graph...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036561ee",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of GraphSAGE\n",
    "\n",
    "### Inductive Learning\n",
    "\n",
    "The key innovation of GraphSAGE is its inductive learning approach, which allows the model to generate embeddings for previously unseen nodes. This is achieved by sampling and aggregating features from a node's local neighborhood.\n",
    "\n",
    "### Neighborhood Sampling\n",
    "\n",
    "For each node \\(v\\), GraphSAGE samples a fixed-size set of neighbors \\(\\mathcal{N}(v)\\). This sampling process is crucial for scaling the model to large graphs, as it reduces the computational burden by focusing on a subset of neighbors.\n",
    "\n",
    "\\[\n",
    "\\mathcal{N}(v) = \\text{Sample}(\\mathcal{N}(v), K)\n",
    "\\]\n",
    "\n",
    "Where \\(K\\) is the number of neighbors to sample.\n",
    "\n",
    "### Aggregation Function\n",
    "\n",
    "GraphSAGE employs an aggregation function to combine the features of the sampled neighbors. Several aggregation functions can be used, including:\n",
    "\n",
    "1. **Mean Aggregation**:\n",
    "\n",
    "\\[\n",
    "h_{\\mathcal{N}(v)}^{(k)} = \\text{mean}\\left(\\left\\{h_u^{(k-1)}, \\forall u \\in \\mathcal{N}(v)\\right\\}\\right)\n",
    "\\]\n",
    "\n",
    "2. **LSTM Aggregation** (with LSTM as the aggregation function):\n",
    "\n",
    "\\[\n",
    "h_{\\mathcal{N}(v)}^{(k)} = \\text{LSTM}\\left(\\left\\{h_u^{(k-1)}, \\forall u \\in \\mathcal{N}(v)\\right\\}\\right)\n",
    "\\]\n",
    "\n",
    "3. **Pooling Aggregation**:\n",
    "\n",
    "\\[\n",
    "h_{\\mathcal{N}(v)}^{(k)} = \\text{max}\\left(\\sigma\\left(W_{\\text{pool}} h_u^{(k-1)} + b_{\\text{pool}}\\right)\\right), \\forall u \\in \\mathcal{N}(v)\n",
    "\\]\n",
    "\n",
    "Where \\(W_{\\text{pool}}\\) and \\(b_{\\text{pool}}\\) are learnable parameters, and \\(\\sigma\\) is a non-linear activation function.\n",
    "\n",
    "### Update Function\n",
    "\n",
    "After aggregating the neighborhood features, GraphSAGE updates the node's representation by combining the aggregated features with the node's own features:\n",
    "\n",
    "\\[\n",
    "h_v^{(k)} = \\sigma\\left(W^{(k)} \\cdot \\text{concat}\\left(h_v^{(k-1)}, h_{\\mathcal{N}(v)}^{(k)}\\right)\\right)\n",
    "\\]\n",
    "\n",
    "Where \\(W^{(k)}\\) is a learnable weight matrix, and \\(\\sigma\\) is a non-linear activation function.\n",
    "\n",
    "### Final Layer\n",
    "\n",
    "The final node embeddings are generated after \\(K\\) iterations of the aggregation and update process. For node classification tasks, a softmax function is applied to the final node embeddings to predict the class probabilities:\n",
    "\n",
    "\\[\n",
    "Z = \\text{softmax}(H^{(K)})\n",
    "\\]\n",
    "\n",
    "Where \\(H^{(K)}\\) is the matrix of final node embeddings, and \\(Z\\) is the matrix of predicted class probabilities.\n",
    "\n",
    "### Training\n",
    "\n",
    "GraphSAGE is trained using gradient-based optimization techniques, with the cross-entropy loss function commonly used for node classification tasks:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_L} y_i \\log(Z_i)\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathcal{V}_L \\) is the set of labeled nodes, \\( y_i \\) is the true label, and \\( Z_i \\) is the predicted probability for node \\( i \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c79c82",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of GraphSAGE using TensorFlow and Keras. This implementation will demonstrate how to build a GraphSAGE model for node classification on a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "class GraphSAGELayer(layers.Layer):\n",
    "    def __init__(self, output_dim, aggregator='mean', **kwargs):\n",
    "        super(GraphSAGELayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.aggregator = aggregator\n",
    "        self.weight = None\n",
    "        self.aggregator_layer = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(shape=(input_shape[0][-1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        if self.aggregator == 'lstm':\n",
    "            self.aggregator_layer = layers.LSTM(self.output_dim, return_sequences=True)\n",
    "        elif self.aggregator == 'pool':\n",
    "            self.aggregator_layer = layers.Dense(self.output_dim, activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, neighbors = inputs\n",
    "        if self.aggregator == 'mean':\n",
    "            agg_neighbors = tf.reduce_mean(neighbors, axis=1)\n",
    "        elif self.aggregator == 'lstm':\n",
    "            agg_neighbors = self.aggregator_layer(neighbors)\n",
    "            agg_neighbors = agg_neighbors[:, -1, :]\n",
    "        elif self.aggregator == 'pool':\n",
    "            agg_neighbors = self.aggregator_layer(neighbors)\n",
    "            agg_neighbors = tf.reduce_max(agg_neighbors, axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown aggregator: {self.aggregator}\")\n",
    "        \n",
    "        h = tf.concat([x, agg_neighbors], axis=1)\n",
    "        h = tf.matmul(h, self.weight)\n",
    "        return tf.nn.relu(h)\n",
    "\n",
    "def build_graphsage(input_dim, hidden_dim, output_dim, aggregator, num_nodes, neighbor_samples):\n",
    "    features = layers.Input(shape=(input_dim,))\n",
    "    neighbors = layers.Input(shape=(neighbor_samples, input_dim))\n",
    "    \n",
    "    x = GraphSAGELayer(hidden_dim, aggregator=aggregator)([features, neighbors])\n",
    "    outputs = layers.Dense(output_dim, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[features, neighbors], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "input_dim = 10   # Example input feature dimension\n",
    "hidden_dim = 16  # Number of hidden units\n",
    "output_dim = 3   # Number of output classes\n",
    "num_nodes = 100  # Number of nodes in the graph\n",
    "neighbor_samples = 5  # Number of neighbors sampled\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_graphsage(input_dim, hidden_dim, output_dim, 'mean', num_nodes, neighbor_samples)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x_train = np.random.rand(num_nodes, input_dim)\n",
    "neighbors = np.random.rand(num_nodes, neighbor_samples, input_dim)\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randint(output_dim, size=(num_nodes,)))\n",
    "\n",
    "# Train the model\n",
    "model.fit([x_train, neighbors], y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d499e7",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of GraphSAGE\n",
    "\n",
    "### Advantages\n",
    "- **Inductive Learning**: GraphSAGE's ability to generalize to unseen nodes makes it particularly useful for dynamic or evolving graphs where new nodes are constantly being added.\n",
    "- **Scalability**: By sampling a fixed number of neighbors, GraphSAGE scales well to large graphs, making it feasible to train on real-world graph data.\n",
    "- **Flexibility**: GraphSAGE can incorporate various types of neighborhood aggregation functions, allowing it to be adapted to different graph structures and tasks.\n",
    "\n",
    "### Disadvantages\n",
    "- **Information Loss**: The sampling process can lead to information loss, as not all neighbors are considered during aggregation, which may affect the model's performance.\n",
    "- **Complexity in Tuning**: The need to choose the right aggregator and sampling strategy adds complexity to the model's design and tuning process.\n",
    "- **Computational Overhead**: While GraphSAGE is scalable, the need to sample and aggregate neighbors during training can still introduce computational overhead, particularly for large graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0887cd",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "GraphSAGE introduced a scalable and flexible approach to graph neural networks by enabling inductive learning on large graphs. Its ability to generalize to unseen nodes and scale to real-world data has made it a popular choice for various applications, including social network analysis, recommendation systems, and biological networks. However, the model's complexity and potential for information loss during neighborhood sampling present challenges that need to be carefully managed. Despite these challeng...\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
