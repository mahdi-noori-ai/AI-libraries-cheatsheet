{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b57e11",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Attention Network (GAT): A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Graph Attention Networks (GATs), including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d20e3",
   "metadata": {},
   "source": [
    "\n",
    "## History of Graph Attention Networks (GATs)\n",
    "\n",
    "Graph Attention Networks (GATs) were introduced by Petar Veličković et al. in their 2017 paper \"Graph Attention Networks.\" GATs were designed to address some of the limitations of Graph Convolutional Networks (GCNs), particularly the inability of GCNs to effectively capture the importance of different neighboring nodes in the graph. GATs introduced the concept of attention mechanisms to graph neural networks, allowing the model to learn different weights for different neighbors, thereby improving the ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d2a0f",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Graph Attention Networks\n",
    "\n",
    "### Attention Mechanism in GATs\n",
    "\n",
    "The core idea of GATs is to apply attention mechanisms to graph data, allowing the model to assign different weights to different neighboring nodes based on their importance. The attention mechanism in GATs is defined as follows:\n",
    "\n",
    "1. **Self-Attention on Nodes**: For each node \\(i\\), we compute a pairwise attention score with its neighboring nodes \\(j\\) using their feature vectors \\(h_i\\) and \\(h_j\\):\n",
    "\n",
    "\\[\n",
    "e_{ij} = \\text{LeakyReLU}(\\mathbf{a}^\\top [\\mathbf{W} h_i \\, \\| \\, \\mathbf{W} h_j])\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\mathbf{W}\\) is a shared weight matrix.\n",
    "- \\(\\mathbf{a}\\) is the attention vector.\n",
    "- \\(\\|\\) denotes concatenation.\n",
    "- \\(e_{ij}\\) is the attention score between nodes \\(i\\) and \\(j\\).\n",
    "\n",
    "2. **Normalization**: The attention scores are normalized using a softmax function to ensure that they sum to one across the neighbors of node \\(i\\):\n",
    "\n",
    "\\[\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n",
    "\\]\n",
    "\n",
    "3. **Weighted Sum of Neighbor Features**: The normalized attention coefficients \\(\\alpha_{ij}\\) are used to compute a weighted sum of the features of node \\(i\\)'s neighbors:\n",
    "\n",
    "\\[\n",
    "h_i' = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W} h_j\\right)\n",
    "\\]\n",
    "\n",
    "Where \\(\\sigma\\) is a non-linear activation function, such as ReLU.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "GATs also incorporate multi-head attention, where multiple independent attention mechanisms (heads) are applied, and their outputs are either concatenated or averaged:\n",
    "\n",
    "\\[\n",
    "h_i' = \\|_{k=1}^{K} \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(k)} \\mathbf{W}^{(k)} h_j\\right)\n",
    "\\]\n",
    "\n",
    "Or:\n",
    "\n",
    "\\[\n",
    "h_i' = \\text{Mean}_{k=1}^{K} \\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(k)} \\mathbf{W}^{(k)} h_j\\right)\n",
    "\\]\n",
    "\n",
    "Where \\(K\\) is the number of attention heads.\n",
    "\n",
    "### Final Layer\n",
    "\n",
    "In a typical GAT used for node classification, the final layer is a softmax function that outputs a probability distribution over the possible classes for each node:\n",
    "\n",
    "\\[\n",
    "Z = \\text{softmax}(H')\n",
    "\\]\n",
    "\n",
    "Where \\(Z\\) is the matrix of predicted class probabilities for each node.\n",
    "\n",
    "### Training\n",
    "\n",
    "GATs are trained using gradient-based optimization techniques, with the cross-entropy loss function commonly used for node classification tasks:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_L} y_i \\log(Z_i)\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathcal{V}_L \\) is the set of labeled nodes, \\( y_i \\) is the true label, and \\( Z_i \\) is the predicted probability for node \\( i \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee982b",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of a Graph Attention Network (GAT) using TensorFlow and Keras. This implementation will demonstrate how to build a GAT for node classification on a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=1, **kwargs):\n",
    "        super(GraphAttention, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = [self.add_weight(shape=(2 * output_dim, 1), initializer='glorot_uniform', trainable=True) for _ in range(num_heads)]\n",
    "        self.kernel = self.add_weight(shape=(output_dim, output_dim), initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, adjacency = inputs\n",
    "        features = tf.matmul(x, self.kernel)\n",
    "        outputs = []\n",
    "\n",
    "        for head in self.attention_heads:\n",
    "            attn_coeffs = []\n",
    "            for i in range(features.shape[0]):\n",
    "                e_ij = tf.reduce_sum(tf.nn.leaky_relu(tf.matmul(tf.concat([features[i], features], axis=-1), head)), axis=1)\n",
    "                attention = tf.nn.softmax(e_ij, axis=0)\n",
    "                attn_coeffs.append(attention)\n",
    "\n",
    "            attn_coeffs = tf.stack(attn_coeffs)\n",
    "            h_prime = tf.matmul(attn_coeffs, features)\n",
    "            outputs.append(h_prime)\n",
    "\n",
    "        output = tf.concat(outputs, axis=-1) if self.num_heads > 1 else outputs[0]\n",
    "        return output\n",
    "\n",
    "def build_gat(input_dim, output_dim, num_heads, num_nodes):\n",
    "    adjacency = layers.Input(shape=(num_nodes,), sparse=True)\n",
    "    features = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    x = GraphAttention(output_dim, num_heads)([features, adjacency])\n",
    "    x = layers.ReLU()(x)\n",
    "    x = GraphAttention(output_dim, num_heads)([x, adjacency])\n",
    "    outputs = layers.Softmax()(x)\n",
    "    \n",
    "    model = models.Model(inputs=[features, adjacency], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "input_dim = 10   # Example input feature dimension\n",
    "output_dim = 3   # Number of output classes\n",
    "num_heads = 8    # Number of attention heads\n",
    "num_nodes = 100  # Number of nodes in the graph\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_gat(input_dim, output_dim, num_heads, num_nodes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x_train = np.random.rand(num_nodes, input_dim)\n",
    "adjacency = np.random.rand(num_nodes, num_nodes)\n",
    "adjacency = (adjacency + adjacency.T) / 2  # Make adjacency symmetric\n",
    "adjacency[adjacency < 0.5] = 0  # Sparsify\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randint(output_dim, size=(num_nodes,)))\n",
    "\n",
    "# Train the model\n",
    "model.fit([x_train, adjacency], y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4dfd9",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Graph Attention Networks (GATs)\n",
    "\n",
    "### Advantages\n",
    "- **Learnable Attention Weights**: GATs allow the model to learn different weights for different neighbors, improving the model's ability to focus on the most important parts of the graph.\n",
    "- **Applicability to Various Graphs**: GATs can be applied to both homogeneous and heterogeneous graphs, making them versatile for different types of data.\n",
    "- **State-of-the-Art Performance**: GATs have achieved state-of-the-art results on several benchmark tasks, demonstrating their effectiveness.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Complexity**: The attention mechanism in GATs increases the computational complexity, particularly when dealing with large graphs.\n",
    "- **Overfitting**: Due to the high capacity of the attention mechanism, GATs may be prone to overfitting, especially on small datasets.\n",
    "- **Scalability Challenges**: While GATs can be scaled to large graphs, the increased complexity and memory requirements can be challenging to manage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c3b62",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Graph Attention Networks (GATs) represent a significant advancement in the field of graph neural networks by introducing attention mechanisms that allow the model to learn the importance of different neighboring nodes. This capability has enabled GATs to achieve state-of-the-art performance on various tasks, including node classification and link prediction. However, the increased computational complexity and potential for overfitting present challenges that need to be addressed. Despite these challenges...\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
