{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e15fff",
   "metadata": {},
   "source": [
    "\n",
    "# BERT-Base: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of BERT-Base (Bidirectional Encoder Representations from Transformers), including its history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0eb4c8",
   "metadata": {},
   "source": [
    "\n",
    "## History of BERT-Base\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) was introduced by Jacob Devlin et al. from Google AI Language in 2018 in the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" BERT was a significant breakthrough in natural language processing (NLP), as it introduced a new paradigm for pre-training language models. The BERT-Base model, with 12 layers (transformer blocks) and 110 million parameters, became the foundation for many state-of-the-art NLP...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296fb8a1",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of BERT-Base\n",
    "\n",
    "### Transformer Architecture\n",
    "\n",
    "BERT is based on the Transformer architecture, which relies on self-attention mechanisms to process input sequences. The core component of BERT is the transformer encoder, which consists of multiple layers of self-attention and feed-forward neural networks.\n",
    "\n",
    "\\[\n",
    "\\text{Self-Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(Q\\) is the query matrix, \\(K\\) is the key matrix, and \\(V\\) is the value matrix, all derived from the input embeddings.\n",
    "- \\(d_k\\) is the dimensionality of the key vectors.\n",
    "\n",
    "### Bidirectional Training\n",
    "\n",
    "BERT's key innovation is its bidirectional training approach. Unlike traditional models that process text either left-to-right or right-to-left, BERT uses masked language modeling (MLM) to predict missing words from both directions.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MLM}} = -\\sum_{i=1}^{n} \\log p(x_i | x_{i-k}, \\dots, x_{i-1}, x_{i+1}, \\dots, x_{i+k})\n",
    "\\]\n",
    "\n",
    "Where \\(x_i\\) is the masked token, and the model predicts it using the surrounding context.\n",
    "\n",
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT also introduces the Next Sentence Prediction (NSP) task, which helps the model understand sentence relationships. The objective is to predict whether two given sentences are consecutive or not.\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{NSP}} = -\\left[ y \\log p(\\text{IsNext}) + (1 - y) \\log p(\\text{NotNext}) \\right]\n",
    "\\]\n",
    "\n",
    "Where \\(y\\) is the binary label indicating whether the second sentence follows the first in the original text.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The total loss function for BERT during pre-training is a combination of the MLM and NSP losses:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{MLM}} + \\mathcal{L}_{\\text{NSP}}\n",
    "\\]\n",
    "\n",
    "### Training\n",
    "\n",
    "BERT-Base is pre-trained on large text corpora using the MLM and NSP tasks. The pre-trained model can then be fine-tuned on specific downstream tasks, such as text classification, named entity recognition, and question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a09a7",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of BERT-Base using the Hugging Face Transformers library. This implementation will demonstrate how to load a pre-trained BERT model and fine-tune it on a sample text classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love programming.\", \"Python is great.\", \"I enjoy machine learning.\", \"BERT is a powerful model.\"]\n",
    "labels = [1, 1, 1, 0]\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(inputs['input_ids'], labels, test_size=0.2)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, torch.tensor(train_labels))\n",
    "val_dataset = torch.utils.data.TensorDataset(val_inputs, torch.tensor(val_labels))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde098b",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of BERT-Base\n",
    "\n",
    "### Advantages\n",
    "- **Contextual Understanding**: BERT captures the context of words bidirectionally, leading to a deeper understanding of language compared to unidirectional models.\n",
    "- **Pre-training and Fine-tuning**: BERT's pre-training on large corpora allows it to be fine-tuned effectively on various downstream tasks, achieving state-of-the-art results.\n",
    "- **Wide Adoption**: BERT has been widely adopted in the NLP community and has set new benchmarks in many NLP tasks.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computationally Intensive**: BERT-Base, with its 110 million parameters, requires significant computational resources for both pre-training and fine-tuning.\n",
    "- **Large Memory Footprint**: The model's size makes it challenging to deploy in resource-constrained environments, such as mobile devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeba47d",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "BERT-Base revolutionized the field of natural language processing by introducing a bidirectional training approach that captures deep contextual relationships between words. Its success has led to widespread adoption in both academia and industry, setting new benchmarks across a variety of NLP tasks. Despite its computational demands, BERT-Base remains a foundational model in modern NLP, and its architecture continues to influence the development of new language models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
