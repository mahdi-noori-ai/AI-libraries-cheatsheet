{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c0a0e07",
      "metadata": {
        "id": "0c0a0e07"
      },
      "source": [
        "# NLTK Cheatsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14df61f",
      "metadata": {
        "id": "e14df61f"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24979569",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24979569",
        "outputId": "c5ca8dec-3df6-4ab7-b32c-0a1f1dd25985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9990527",
      "metadata": {
        "id": "a9990527"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "62863f49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62863f49",
        "outputId": "3de19f8b-36b2-43d9-be5f-a5fcaa39c03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['NLTK is a leading platform for building Python programs to work with human language data.']\n",
            "Word Tokenization: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "# Sentence tokenization\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sent_tokens)\n",
        "\n",
        "# Word tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", word_tokens)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee4c2f3b",
      "metadata": {
        "id": "ee4c2f3b"
      },
      "source": [
        "## Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76d39448",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d39448",
        "outputId": "3ff8f257-fdf9-4ed2-cbd8-e5e5c1af9b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Stopwords removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Words:\", filtered_words)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96bd4f8",
      "metadata": {
        "id": "f96bd4f8"
      },
      "source": [
        "## Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cd63fe97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd63fe97",
        "outputId": "f04fb8f4-7d7d-403f-9ad5-1ba8b609e5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency Distribution: <FreqDist with 16 samples and 16 outcomes>\n",
            "Most Common Words: [('NLTK', 1), ('is', 1), ('a', 1), ('leading', 1), ('platform', 1)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Frequency distribution\n",
        "fdist = FreqDist(word_tokens)\n",
        "print(\"Frequency Distribution:\", fdist)\n",
        "print(\"Most Common Words:\", fdist.most_common(5))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d7e783",
      "metadata": {
        "id": "b8d7e783"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ce19f79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ce19f79",
        "outputId": "f6541a87-bfd0-4c10-b115-b70d35fedf80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in word_tokens]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3613e55",
      "metadata": {
        "id": "a3613e55"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "71d5bef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d5bef2",
        "outputId": "0ebe5ae4-9568-45e3-f537-86349c5ca4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bcbc25",
      "metadata": {
        "id": "f0bcbc25"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f85297d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f85297d3",
        "outputId": "c50d7184-ec83-4209-8cd0-cce2bd05db9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Part of speech tagging\n",
        "pos_tags = pos_tag(word_tokens)\n",
        "print(\"Part of Speech Tags:\", pos_tags)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e750122",
      "metadata": {
        "id": "6e750122"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4776dfbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4776dfbe",
        "outputId": "e4170ce6-12c8-4b14-d313-a5ef7fcf011b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: (S\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  leading/VBG\n",
            "  platform/NN\n",
            "  for/IN\n",
            "  building/VBG\n",
            "  (PERSON Python/NNP)\n",
            "  programs/NNS\n",
            "  to/TO\n",
            "  work/VB\n",
            "  with/IN\n",
            "  human/JJ\n",
            "  language/NN\n",
            "  data/NNS\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Named entity recognition\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\", named_entities)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e47ff73",
      "metadata": {
        "id": "2e47ff73"
      },
      "source": [
        "## Synonyms and Antonyms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a63c6ffb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a63c6ffb",
        "outputId": "49cde80e-d2b9-47c5-e112-bceebed9d8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms of 'good': {'beneficial', 'sound', 'respectable', 'honest', 'in_effect', 'undecomposed', 'skillful', 'commodity', 'adept', 'trade_good', 'well', 'soundly', 'salutary', 'upright', 'secure', 'practiced', 'in_force', 'full', 'unspoilt', 'serious', 'skilful', 'effective', 'expert', 'goodness', 'honorable', 'safe', 'just', 'right', 'ripe', 'proficient', 'good', 'near', 'unspoiled', 'thoroughly', 'estimable', 'dear', 'dependable'}\n",
            "Antonyms of 'good': {'evilness', 'evil', 'badness', 'bad', 'ill'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Synonyms and antonyms\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"good\"):\n",
        "    for lemma in syn.lemmas():\n",
        "        synonyms.append(lemma.name())\n",
        "        if lemma.antonyms():\n",
        "            antonyms.append(lemma.antonyms()[0].name())\n",
        "\n",
        "print(\"Synonyms of 'good':\", set(synonyms))\n",
        "print(\"Antonyms of 'good':\", set(antonyms))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf9502c",
      "metadata": {
        "id": "fcf9502c"
      },
      "source": [
        "## WordNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "002a9306",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002a9306",
        "outputId": "9179dc75-24ff-402d-cf63-4301d8eaf877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synsets of 'program': [Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n",
            "Definition of the first synset: a series of steps to be carried out or goals to be accomplished\n",
            "Examples of the first synset: ['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# WordNet\n",
        "synsets = wordnet.synsets(\"program\")\n",
        "print(\"Synsets of 'program':\", synsets)\n",
        "print(\"Definition of the first synset:\", synsets[0].definition())\n",
        "print(\"Examples of the first synset:\", synsets[0].examples())\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQq29JSGrmAg"
      },
      "id": "kQq29JSGrmAg",
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}