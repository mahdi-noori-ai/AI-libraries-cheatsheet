{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233d19e2",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Transformer: A Comprehensive Overview\n",
    "\n",
    "This notebook provides an in-depth overview of Graph Transformers, including their history, mathematical foundation, implementation, usage, advantages and disadvantages, and more. We'll also include visualizations and a discussion of the model's impact and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab9d71",
   "metadata": {},
   "source": [
    "\n",
    "## History of Graph Transformers\n",
    "\n",
    "Graph Transformers represent an extension of the Transformer architecture, which was originally designed for sequence data, to graph-structured data. Since the introduction of the Transformer model in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, researchers have explored its applications beyond natural language processing, leading to the development of various adaptations for graph data. The Graph Transformer model applies self-attention mechanisms to nodes and edges in a graph, allo...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f491a6f",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical Foundation of Graph Transformers\n",
    "\n",
    "### Self-Attention Mechanism on Graphs\n",
    "\n",
    "The core component of Graph Transformers is the self-attention mechanism, which computes attention scores between nodes and their neighbors, allowing the model to capture dependencies across the graph.\n",
    "\n",
    "Given a set of node features \\(X = \\{x_1, x_2, \\dots, x_N\\}\\), the self-attention mechanism computes the attention score \\( \\alpha_{ij} \\) between nodes \\(i\\) and \\(j\\) as:\n",
    "\n",
    "\\[\n",
    "e_{ij} = \\frac{(\\mathbf{W}_q x_i)^\\top (\\mathbf{W}_k x_j)}{\\sqrt{d_k}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{W}_q \\) and \\( \\mathbf{W}_k \\) are weight matrices for the query and key, respectively.\n",
    "- \\( d_k \\) is the dimension of the key vectors.\n",
    "\n",
    "The attention scores are then normalized using a softmax function:\n",
    "\n",
    "\\[\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n",
    "\\]\n",
    "\n",
    "### Message Passing\n",
    "\n",
    "After computing the attention scores, the Graph Transformer updates the node features by aggregating messages from the neighbors:\n",
    "\n",
    "\\[\n",
    "h_i' = \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} (\\mathbf{W}_v x_j)\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathbf{W}_v \\) is the weight matrix for the value vectors.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Similar to the original Transformer model, Graph Transformers use multi-head attention to capture different types of relationships:\n",
    "\n",
    "\\[\n",
    "\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\mathbf{W}^O\n",
    "\\]\n",
    "\n",
    "Where each attention head \\( \\text{head}_i \\) is computed independently.\n",
    "\n",
    "### Graph Structure Encoding\n",
    "\n",
    "To incorporate the structural information of the graph, positional encodings or graph Laplacian eigenvectors can be added to the node features:\n",
    "\n",
    "\\[\n",
    "\\tilde{X} = X + \\text{PE}\n",
    "\\]\n",
    "\n",
    "Where \\( \\text{PE} \\) represents the positional encoding or structural information.\n",
    "\n",
    "### Final Layer and Training\n",
    "\n",
    "For node classification tasks, a softmax function is applied to the final node representations to output class probabilities:\n",
    "\n",
    "\\[\n",
    "Z = \\text{softmax}(H')\n",
    "\\]\n",
    "\n",
    "The model is trained using a cross-entropy loss function:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = -\\sum_{i \\in \\mathcal{V}_L} y_i \\log(Z_i)\n",
    "\\]\n",
    "\n",
    "Where \\( \\mathcal{V}_L \\) is the set of labeled nodes, \\( y_i \\) is the true label, and \\( Z_i \\) is the predicted probability for node \\( i \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c07e23",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation in Python\n",
    "\n",
    "We'll implement a basic version of a Graph Transformer using TensorFlow and Keras. This implementation will demonstrate how to build a Graph Transformer for node classification on a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "class GraphTransformerLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, num_heads=1, **kwargs):\n",
    "        super(GraphTransformerLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_heads = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=output_dim) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, adjacency = inputs\n",
    "        attention_outputs = []\n",
    "\n",
    "        for head in self.attention_heads:\n",
    "            attn_output = head(x, x)\n",
    "            attention_outputs.append(attn_output)\n",
    "\n",
    "        output = tf.concat(attention_outputs, axis=-1) if self.num_heads > 1 else attention_outputs[0]\n",
    "        return output\n",
    "\n",
    "def build_graph_transformer(input_dim, output_dim, num_heads, num_nodes):\n",
    "    adjacency = layers.Input(shape=(num_nodes,), sparse=True)\n",
    "    features = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    x = GraphTransformerLayer(output_dim, num_heads)([features, adjacency])\n",
    "    x = layers.ReLU()(x)\n",
    "    x = GraphTransformerLayer(output_dim, num_heads)([x, adjacency])\n",
    "    outputs = layers.Softmax()(x)\n",
    "    \n",
    "    model = models.Model(inputs=[features, adjacency], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Parameters\n",
    "input_dim = 10   # Example input feature dimension\n",
    "output_dim = 3   # Number of output classes\n",
    "num_heads = 8    # Number of attention heads\n",
    "num_nodes = 100  # Number of nodes in the graph\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_graph_transformer(input_dim, output_dim, num_heads, num_nodes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for demonstration\n",
    "x_train = np.random.rand(num_nodes, input_dim)\n",
    "adjacency = np.random.rand(num_nodes, num_nodes)\n",
    "adjacency = (adjacency + adjacency.T) / 2  # Make adjacency symmetric\n",
    "adjacency[adjacency < 0.5] = 0  # Sparsify\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randint(output_dim, size=(num_nodes,)))\n",
    "\n",
    "# Train the model\n",
    "model.fit([x_train, adjacency], y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19293e16",
   "metadata": {},
   "source": [
    "\n",
    "## Pros and Cons of Graph Transformers\n",
    "\n",
    "### Advantages\n",
    "- **Flexibility and Expressiveness**: Graph Transformers can capture complex dependencies and relationships in graph data, making them highly flexible and expressive.\n",
    "- **Applicability to Various Graph Structures**: Graph Transformers can be applied to both homogeneous and heterogeneous graphs, making them versatile for different types of data.\n",
    "- **State-of-the-Art Performance**: Graph Transformers have achieved state-of-the-art results on several benchmark tasks, demonstrating their effectiveness.\n",
    "\n",
    "### Disadvantages\n",
    "- **Computational Complexity**: The self-attention mechanism in Graph Transformers increases the computational complexity, particularly when dealing with large graphs.\n",
    "- **Scalability Challenges**: Scaling Graph Transformers to very large graphs can be challenging due to the increased memory and computational requirements.\n",
    "- **Complexity in Implementation**: The model's complexity requires careful tuning of hyperparameters and can be more challenging to implement and debug compared to traditional graph neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff92fd0f",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Graph Transformers represent a significant advancement in the field of graph neural networks by introducing self-attention mechanisms that allow the model to learn complex relationships within the graph. This capability has enabled Graph Transformers to achieve state-of-the-art performance on various tasks, including node classification, link prediction, and graph classification. However, the increased computational complexity and scalability challenges present hurdles that need to be carefully managed....\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
